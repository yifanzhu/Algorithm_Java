Chapter 7 Information PresentationAbstract   This chapter focuses on how to present the results of the search.  It is specific to information retrieval and search results rather than being a general dis-cussion on information visualization or presentation.  The various scenarios of in-formation presentation are defined along with aspects of their presentation.    Vis-ualization of search results is organized by linear lists, clustering views, network views and time line presentation.  Collaborative filtering is discussed as an alter-native mechanism for helping a user navigate to potentially useful information not found in a hit list.  When displaying an individual item highlighting and other technologies can help focus a user’s attention on the sections of an item most like-ly to be relevant to their information need.  Displays associated with multimedia information retrieval have parallelisms with textual display but have their set of unique characteristics.7.1  Information Presentation Introduction	Once a search has been completed the system has identified a “hit list” of items in ranked order based upon the most likely to be relevant.  The next step is to present the information to the user.  The presentation has a significant impact on the user’s ability to find the information they are looking for.   There are two stages for information presentation.  The first is how the hit list is presented so the user can determine if it may contain potential information to answer their infor-mation need.  The second is how individual items are presented to the user from which they will look for specific information.  There are a few scenarios where the hit list presentation could answer the user’s question without the need to look at the individual items – but this is the exception case.	There are multiple techniques applicable to each of the stages based upon what information a user is looking for.  But in all cases the goal is to leverage off the humans ability to visually process information to optimize the presentation to assist in identifying the needed information.  Historically the primary presentation has been a linear list of potential information.  For users in the 1980s and 1990s that was adequate because their experience followed that paradigm.  But in the 2000s with the availability of sophisticated virtual games, the Internet and com-munications devices the users are expecting more sophisticated presentation methods. In all cases the goal is to organize the information’s presentation to al-low the user to detect patterns that help focus on the most likely location for the interest.7.2  Presentation of the Hits	The hit list contains the items of potential interest.  What is less obvious is how long the hit list is.  Most systems display the hit list as a sequential list of hits and at the user at most reviews the first two pages of hits (20-30 hits).  The system will display a count of the total number of hits.  In most systems they have not been uniquely identified but instead the total hit count is an estimate is created based upon the subset of hits identified for actual display.	Looking at textual items there are four major processes that can be used to display the hit list, each satisfying a different information need.  The first is the sequential list that is in ranked order based upon the users query and a confidence level (weight).  It is the most common hit display.  The next is a cluster visualiza-tion that quite often is hierarchical in nature (see HACM from Chapter 6).  A net-work display is used when entities are identifiable and the relationships are the goal.  Less often used is a time line display.7.2.1  Sequential Listing of HitsThe details on how the weights are calculated are discussed in Chapter 5.  This display is the most common display because it is the easiest to present.  The display of each hit can be broken down into three major subcomponents each of which is designed to help the user decide if they want to select and display that hit.  The first line of the hit is the “title”.  This is usually extracted from the Title of the page that is being referenced or the title of the item.  The next part is the “snippet” that is text that comes from that hit item (e.g., web page) that is designed to provi-de the semantic information to help the user understand what the item is about.  The simplest snippet is text taken the first few sentences of the item or if there is a metadata Description tag to use that information.  If there specific zones such as an “Abstract” zone that also could be used.  The advantage to this definition of a snippet is that it is static and thus can be predefined allowing for faster display of the Hit list.  The snippet can come from anywhere in the document.  A more fo-cused solution will have the snippet come from specific sentence fragments that the search term(s) was found.  This could lead to many different possible snippets that could be extracted.  Using the weights of the hit terms and proximity of the hit terms within text segments (possible snippets) can help in determining which snippets to display.  The length of the snippet needs to be limited because the longer the snippet the fewer hits can be displayed on a display screen.  Ideally the snippet would include a summarization focusing on the information need (query) of the item but that is unrealistic.  Text summarization is discussed in section 7.3.2 where the difficulties are described.  In all cases if any of the user’s search terms are in the displayed text they are usually highlighted to draw attention to them.  The final most critical part of the hit listing is a link to the item.Once past the basic hit list display data there is optional data that may be displayed based upon how complex the search engine and system is?  In general, its specific data that is related to the nature of the particular item being displayed.   For example if the item being displayed is recognized with an entity type (one of the advantages of doing entity identification) that suggests it has a physical loca-tion and if the address is available – links are provided to a map showing where it is located.  If the item is a company links could be provided to show the stock val-ue of the company.  The goal of this process is to not only present the user with adequate information to understand what the link to the item will likely return but also provide links to other information that frequently is of interest when looking for an item.  Thus the rationale behind showing a link to the map for an address of a business is that quite often the users real search is for where a company is locat-ed but their search query may only have the company name in it.A sequential listing of hits is most useful when a user is looking for a specific fact or gathering background on a very focused topic.  Given the specifici-ty of the topic and that the user wants some very focused data the sequential ranked presentation presents the information in an optimum organized format.  The user will either find the one or two items it will take to answer their question in the first few pages or find additional terms to try a new query and find the in-formation.  The nature of this search is that the user does not need to have an overview of many items to help in locating the information needed.  7.2.2  Cluster View	In some cases the user is looking to understand a more broad topical area.  It could be an area that is new to the user or it could by its nature be a more gen-eral focus.  In this case a sequential list of items will never satisfy the real infor-mation need.  It is too focused on individual items when the user needs to under-stand a more general view of the information to help in focusing on the specific information they need.  For example if the user wants to understand how to invest money, a query on money investment brings up 27,000,000 hits and a number of articles on “investing money”.  The individual items may be useful in starting to get a background but they may never present the scope of the information availa-ble in the system on this topic.  	An alternative approach is to present a visualization of all of the hits or large representative sample.  To do this a clustering algorithm would be applied to the hits (possibly a hierarchical clustering approach) and then the clusters would be presented in a graphical interface to the user.  The clusters would have labels that would in effect be categorizing the different topics that are part of the more general search.  The key to this interface is the ability to provide a meaningful name to the clusters.  The user would then be able to focus on the individual clus-ters looking in more depth on the clusters within the first set of clusters and even-tually look at specific items of interest.	This type of an interface is typically not provided because of the signifi-cant overhead in dynamically clustering the hits from a search.  In many cases the cluster software will be an additional application and the user can request that the hits be exported to that package where the clustering takes place.  This is quite of-ten limited to hundreds of thousand hits (e.g., In-Spire from PNNL see its Theme View).  A two or three dimensional display is quite often used with the heights or densities of the display indicating topical areas.  In a 2-dimension display (e.g., scattergram) the density of points indicate topical areas where each point repre-sents another item.  For example in Figure 7.1 there are some examples of a scat-ter gram where you can see by the densities the number of items that are discuss-ing the different topics.  In addition for this example the size of the point for each item indicates the confidence level that it is associated with cluster.  The user can then focus on the overall set of topical areas and then investigate which ones most closely map to the users information need.  The second example in Figure 7.2 shows a 3 dimensional visualization.  In this case each topic is associated with each of the three dimensional “hills” in the picture.  In addition the height and size of each hill indicates how many items are associated with that area.	Filtering and zooming are the most important parts for an interface such as this. Zooming follows the general rule of visualization design where the more general higher level information visualization is presented first and then the user can “zoom in” or focus on particular parts of the display to expand the details for that segment.  The other important capability is filtering.  This allows a user to specify some criteria that they want to be removed from the current display.  This allows filtering out items that may be confusing the display and not helping the user to find the desired information. For example filtering out information of for-eign currency exchange could reduce the complexity of a scattergram on invest-ments.  Quite often a time filter is also very useful where items older than a certain date can be filtered out.  This can change the display from an overall graphical view of a topical area to just the most recent information on a topical area.   Figure 7.1 Example of Clustering Scattergram  Figure 7.2 Example of Theme View 3 Dimensional Display1	One of the challenges in creating a cluster view is how to name the clus-ters.  The simplest technique is to find the highest weighted terms that are found across the cluster and then use the top few words as a label for the cluster.  This can lead to errors when the cluster is representing a large concept whose label is not frequently found in the specific processing tokens of the items in the cluster.  Another approach is to try and abstract out the cluster name.  For example one approach could take clusters as the input and generates appropriate labels for the clusters using a reference database. The objective is to take the highest weighted terms and then look up the different word senses for those words and also look for more abstract concepts that the words are part of.  Then based upon the set of words that come from the look-up a weighting algorithm can be used to select the most important labels to be associated with the clusters.  This helps in consolidating synonyms or near synonym words into a single word representation and allows the cluster names to come from words that are not in the cluster.  If the cluster is a hierarchical cluster structure then the process works the same except the children labels can be inherited to the parent clusters and merged into a higher cluster label.  The advantage of working with a Natural Language Processing indexing scheme is that it will provide additional information that could be used in determining the name to be associated with a cluster.  In this case the part of speech and usage of the words could be an additional weighting factor in determining the weight of individual words in labeling the cluster.  For example a Subject would have a lot more weight that a word that is an adjective or direct object in use. 	In many cases clustering software starts with a reprocessing of the original items to create an index more optimized for the performing the clustering and manipulating the display of the cluster.  That is why there are limits placed on the number of items clustered (i.e., in the tens to hundreds of thousands).  Clustering is very different than indexing data for a query in that the final goal is a visual presentation of all of the data to assist the user in better understanding the overall information content of all of the data.  In clustering the original concepts of applying Ziph’s Law to reduce the complexity of the index re-emerges as a viable constraint.  Ziph’s Law says the frequency of occurrence of words in a set of text follows and exponential rule where a small percentage of terms are found very frequently and a very large number of terms are only found a few times in the text.  When computer computation resources was limited many years ago that was proposed as a way to limit what processing tokens were created and indexed (i.e., eliminating the high frequency and low frequency words).  When the goal is to create a visualization of a large number of items that rule currently applies.  High frequency words that are found in most of the items blur the differences between items and do not help in clustering where the objective is to show the unique general topics of the items.  The low frequency words that are only found in a few items does not help in clustering in that they only help define single items that will be at the edges or standing alone in a cluster space.  Thus when indexing for clustering it’s possible to reduce the dimensionality (i.e., number of processing tokens) that are kept and used in the clustering from millions down to hundreds to a few thousands without seriously degrading the information value of the cluster display.   The goal is to select those words that provide the best discrimination value to help in defining the clusters. This is usually a few thousands terms that are primary needed to represent the major concepts across a corpora.  That does not mean that all terms are not kept for “filtering” the current or new set of items that are used in creating the cluster display.  But when that subset of items is selected the reduced index can then be used to create the new clusters.  In creating the processing tokens for the clustering quite often units other than individual words are used.  Thus the system may also treat multiple words or even sentences as a unit to index to help in the clustering.   In a sense this approach is driving more towards the idea of “concept indexing” such as use of Latent Semantic Indexing as a way to reduce the dimensionality of the original set of items to hundreds of processing tokens that represent the concepts in the items.  	Once the reduced set of processing tokens is defined then Luhn’s concept of co-occurrence of processing tokens with the same items suggesting those items are talking about the same concept applies.  As discussed in Chapter 6 there are significant performance issues when trying to do the clustering using the full set of termi and termj co-occurrence.  That is why the K-means algorithm (i.e. starting with an initial set of clusters and then reassigning items to that set in an iterative process) is often applied to define the clusters.  	Once the clusters have been determined then there is the issue of mapping the clusters to a 2-dimensional or 3-dimensional representation that can be shown on visualization.  One of the common ways of doing that is using Principal Component Analysis (PCA).  It can reduce the n-dimensional processing token space into a few dimensions for display.  It dates back to 1901 when it was first introduced by Karl Pearson.  It uses orthogonal vector decomposition of the multivariate data.  These are called “principal components and is accomplished by applying single value decomposition of the data matrix (see Latent Semantic Indexing in Chapter 4).  By choosing the top 2 or 3 variables it provides a “shadow” of the higher n-dimension space into a 2 or 3 dimension space which can then be used to create the display.  PCA is mathematically defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on. PCA is theoretically the optimum transform for given data in least squares terms (Jolliffe-2002).7.2.3 Network ViewThe network view of a hit list is the best display of search results where the user is looking for relationships between entities.  The existence of a relation-ship is identified within an item.  An example would be if a user wants to see what companies would be effected if General Motors Corporation (GMC) car company goes bankrupt.  The system is trying to determine relationships between GMC and other companies.  The result would be a multinode network where each node rep-resents a company or subnetwork of companies and the links indicate a relation-ship between the companies.  There could be associative relationships where one company is dependent upon GMC and another company is dependent upon that company.  This is one of the examples where part of the user’s information need could be answered by the display which should be displayed in a network graph form. Figure 7.3 shows what a network graph might look like for this information need. The network graph representation summarizes the specific examples the sys-tem has determined to suggest relationships.  The width of the connecting lines in-dicates the confidence level of that connection.  By clicking on any of the lines the system would display all of the specific items that went into defining that line.  In this example there are two nodes that really represent additional lower level net-works (e.g., dealers and Charities).  Clicking on either of those nodes would ex-pand out the all of the sub-nodes in that node.  The algorithms to determine the re-lationships will likely contain errors in accuracy due to the challenges in Information retrieval that have been discussed throughout this book.  The network graph should be looked at another way of organizing the hits, but for the user to make an accurate decision the user needs to review the items that led to the parts of the graph the user feels is important.  In this case the hit items are associated with the lines between the nodes where a relationship was determined from the text of the item. Figure 7.3  Network Diagram example	There are many different algorithms used in determining that two entities are related by an item.  The simplest technique is to use the concept of co-occurrence that has been used in many of the algorithms.  The idea is that if two entities co-occur in many items then those two entities may be related.  A weighting algorithm is:Link Weight (Entityi,Entityj) =  N* (WTEi * WTEj)Where N is a normalization constant and WTEi  is the term frequency weight of the entity within the item.  The term frequency is based upon the fully normalized oc-currence count of the entity within the item versus just the specific name variation of the entity as entered by the user.  If entity normalization (see Chapter 3) is not available for that specific entity then just the entered term would be counted.	The formula above can be adjusted to have more semantic strength in the possibility of the two entities being related by adding a proximity constraint to when the system counts an occurrence of the two entities being in the item that leads to the WTEi value.   Thus a proximity constraint that Term for Entityi  and the term representing Entityj must be within “m” units of each other before it will count towards a linkage can be applied.  In this case “units” could be words, sen-tences, paragraphs or any other meaningful measure.	The statistical approach introduces errors in that the co-occurrence does not guarantee the semantic relationship between the entities just their existence.  A more sophisticated approach would use either the existence of “cue words” or nat-ural language processing to increase the recognition of the strength between the entities when found in an item.  “Cue words” are words that are chosen because they indicate the likelihood of what something is about.  One of the first uses for “cue words” was in detecting sentiment analysis about products on the Internet.  Sentiment analysis is the ability for web crawlers to understand the tone, be it pos-itive or negative, of conversations on the Internet For example Coca Cola might want to see what the “Buzz” is about their latest flavor.  Rather than doing a tele-phone survey they hire a company to crawl and detect when that product is being discussed in a chat room or blogs and what is being said about it.  To detect the sentiment a set of “cue” words are used to determine if the discussion is positive or negative.  This is easier than trying to do full linguistic analysis.  Selection of cue words can be tricky.  For example the word “worse” would usually imply a negative.  Thus someone might have written “New Coke is the worse drink I have had”.  But in the sentence is “I want New Coke in the worse way” it is indicating a positive.  The selection of the right list of “cue words” for a particular relationship and then looking for their presence can help identify better linkages.	The best solution but most complex is to do full linguistic analysis on the sentences where the entity words occur to be sure that the linkage is warranted and the strength of it.  This is the least likely to have errors but is most complex and difficult to provide unless the system is already a natural language index system.	The final product once the link analysis has been performed is a network diagram that has the entities of interest as the nodes and the links as lines between the nodes.  Ideally the strength of the link should be indicated in some way by dif-ferent representations of the line (e.g., width of the line).  The user may have their information need filled by just seeing the network graph.  But the network graph is another way of showing a hit list.  The hits are the items that contributed to the creation of the connecting lines.  The user should have the capability to click on a line and see the items that contributed towards creation of that link.  The user can then display the individual items to see exactly what information was in the item that suggested the relationship.  As with clusters a network graph can be very dense and difficult to understand.  Thus as with clusters the users need the capabil-ity to filter some of the nodes out of the display and to zoom in and navigate around the network.  Zooming in will allow expansion of parts of the network that may be hidden when shown in the context of the complete network.7.2.4 Timeline  PresentationAlthough not as common as the above three presentations, an additional way of showing the hits from a search in some instances can be a time line (actual-ly date/time but it is generalized to just “timeline” even though most presentations are date oriented rather than time within date).  In the right circumstances this can significantly reduce the users time in finding the information needed.  The trouble with ranked hit lists is that the hits are weighted by similarity to the search in ranked order, but time is not typically a factor in that display.  Thus if the highest weighted information is years old it will be presented first.  The most recent in-formation may be presented way down the list.  When the user is looking for a specific question then time may not be as important as similarity to the search.  But in some cases the user is interested in more recent information or how a topi-cal area grew over time.  There are two possible times of interest.  The first is when a particular item was published.  The second is when in time that the item is about.  Originally when GOOGLE first experimented with this concept on their experimental web site they had a more generalized view of the data.  For example if you searched on Automobile view: timeline example you will see a time line with the hits on the time line (see Figure 7.4).  As with any visualization you can zoom in on a part of the time line and have it expanded out in greater detail.   Figure 7.4 Original GOOGLE Time Line CapabilityEventually the business decision of what timeline aspect is most interest-ing by GOOGLE led them to focus the time line on news rather than letting it be too generic.  This then lead to GOOGLE News Timeline option made available on April 20, 2009  (http://newstimeline.googlelabs.com/ ) and search on a news story (example is crash of Air France Plane).  It presents news stories based upon that as a search.  You will notice there are false hits on other airline crashes since this is a ranked retrieval system (see Figure 7.5) Figure 7.5 GOOGLE News Timeline	Central to display of a timeline presentation is the extraction of date and time information in an item as additional metadata to be used in creating the dis-play.  As with all of the other displays this is just a different way of organizing the hits to help the user find the information they are interested in.  The “drilling” down on the time line will eventually lead to the links to specific items for the user to view.  Some of the aspects of the design of a timeline view to help the user are:Time (i.e., date/time) spans should be hierarchical in display in that larger time windows are first displayed and then the user can “drill down” to more de-tailed time periodsAll of an items time references can be displayed by using different sizes for each item to indicate how many time references are in each item (e.g., one time reference may be a single point, multiple may be a rectangle)The display needs simple navigation (e.g., arrows) along the axis for time to allow the user to navigate to new adjacent time periods.7.3 Display of the Item7.3.1  Indicating Search terms in display	Display of the item can also influence how much time it takes a user to locate the information he is interested in.  In particular the user wants to be able to quickly locate the text(s) in the item that was the basis behind its retrieval.  This has historically been accomplished via highlighting the search terms within the text of the item.  Highlighting has been experimented with using colors for the dif-ferent search terms but that can cause difficulties with users that have color recog-nition problems and are thus not generally usable by all.  The most common tech-nique is to highlight/bold the search terms in the text.	For short items that are only a couple of paragraphs long, highlighting is sufficient and lets the user to quickly focus on the areas of potential interest.  But when an item is many pages long then other techniques can add significantly more insight to the user than just highlighting.  One technique that helps the user in quickly orienting his focus is to display in a vertical display along either edge of the item what search terms are found in which lines of text.  Thus the user can quickly scroll down through the item seeing which search term(s) are found to-gether and helping in doing a first level scan to decide where to read in more de-tail.  This technique shows how the information presentation can leverage off of the users visual ability to scan to reduce the time it takes to locate potential infor-mation of value.  	Also with long items where not all of the search terms are on the viewa-ble screen, the ability to jump from the current position to the next occurrence of a specific search term or any of the search terms also is very beneficial to the user.  This is made more powerful if the user can select (filter) in deciding which search terms will be highlighted/jumped between.7.3.2  Text SummarizationIndicating important terms in items help the user focus on the parts of the item that are most likely to be of value to the users information need.  The nature of most items on a particular topic is that there will be significant redundancy in each item describing the same basic facts before it expands upon those facts.  A user in trying to answer their information need spends a lot of time redundantly reading information they are already aware of.  In many cases they will not find any new information in the items they review.  	The solution to this problem is text summarization. Examples of summar-ies that are often part of any item are titles, table of contents, and abstracts with the abstract being the closest to a summarization of the important ideas in an item. The abstract can be used to represent the item for search purposes or as a way for a user to determine the utility of an item without having to read the complete item. There are two classes of text summarization with the first being to summarize the text within an item eliminating redundancy and focusing on the most important in-formation.  The second level is to summarize text between multiple items.  Sum-marization of the item could significantly save the user time in understanding the information in the item.  If the summaries were made short enough they could be used as the snippet in the hit list, reducing further the need for a user to open a complete item to see if it has information of value.  Another factor that can be used in summarization is if the goal is to summarize the complete item or if the summarization should be focused around the topics indicated by the terms in the users query.	The summarization process is typically done in three steps.  First poten-tial material within the item is identified that is important.  Next the material is compressed to eliminate redundancy.  Finally it is paraphrased to make it continu-ous and readable.  The simplest summarization can be extracts of phrases, sen-tences or even paragraphs of an item.  The more complex summarizations are a compiled abstract that eliminates redundancy and summarizes the item.  There are two ways of approaching the summarization process.  The first is to create a sum-marization of the item (or items) based upon the content in the item.  An alterna-tive is to focus the summarization on the parts of the item that are related to the query.  Summarizations can be either “extract” based or “abstract” based.  Extracts are identifying textual units within an item (e.g., sentences) that have high infor-mation value and extracting and concatenating them  Abstract based requires text understanding because it merges ideas into a new textual descriptor.  Endres- Nig-genger in their analysis discovered that users sometimes prefer extract based summaries which allow the user to merge the specifics into an abstract over sys-tem generated abstracts that may tend to diffuse the information in the summary.	Luhn was one of the first to propose automated methods for summariza-tion.  He proposed that the frequency of a word in an item indicates the word may be about a topic within an item.  Clusters of frequent words within a sentence would indicate that sentence would be a good summarizing sentence for the item.   He also felt this should be adjusted by the frequency of the word in the database (e.g., similar to the concept of inverse document frequency but based upon term frequency in database not document frequency).   It is not feasible to automatically generate a coherent narrative summary of an item with proper discourse, abstrac-tion and language usage (Sparck Jones-93).  Restricting the domain of the item can significantly improve the quality of the output (Paice-93, Reimer-88).  The more restricted goals for much of the research is in finding subsets of the item that can be extracted and concatenated (usually extracting at the sentence level) and represents the most important concepts in the item.  There is no guarantee of read-ability as a narrative abstract and it is seldom achieved.  It has been shown that ex-tracts of approximately 20 per cent of the complete item can represent the majority of significant concepts (Morris-92).  Different algorithms produce different sum-maries.  Just as different humans create different abstracts for the same item, au-tomated techniques that generate different summaries does not intrinsically imply major deficiencies between the summaries.  	Another historical method for extraction is based upon the work of Ed-mundson which uses position in the text.  The idea is that important sentences oc-cur in specific locations.  Thus the position of the sentence in a paragraph and the position of the paragraph in the overall text can be used to determine what sen-tences to extract as summary sentences.  The position is not universal but is de-pendent upon the type of an item it is.  For example news items have the important sentences in the beginning.  Scientific items tend to have them at the end.  In addi-tion to using the position, the Title of an item attempts to describe what the item’s core idea is.  Thus any sentences where major words from the title are found should be weighted heavier.  Finally there are “cue” words/phrases as discussed earlier that are indicative of a good summary sentence.  Some examples of them are:  “In summary”, “we have shown that” and “the goal of this”.  Using the above factors and assigning weights to them it’s possible to assign weights to sentences to determine which sentences to extract.	There is no overall theoretic basis for the approaches, leading to many heuristic algorithms. Kupiec et al. are pursuing statistical classification approach based upon a training set reducing the heuristics by focusing on a weighted com-bination of criteria to produce “optimal” scoring scheme (Kupiec-95).  They se-lected the following five feature sets as a basis for their algorithm:Sentence Length Feature that requires sentence to be over five words in lengthFixed Phrase Feature that looks for the existence of phrase “cues” (e.g., “in conclusion)Paragraph Feature that places emphasis on the first ten and last five para-graphs in an item and also the location of the sentences within the para-graphThematic Word Feature that uses word frequencyUppercase Word Feature that places emphasis on proper names and acro-nyms. As with previous experiments by Edmundson, Kupiec et al. discovered that lo-cation based heuristics gives better results than the frequency based features (Ed-mundson-69).  	Extraction is simple, but the problem is how to link extracted sentences and make them one single and meaningful summary.  Abstraction is another main method used for summarization, and it is the main method human using. The spe-cial characteristic is of the abstraction method is that it will include the sentences the original text does not have. In order to build abstraction, people or machines need to understand the original text. Abstraction method has higher intelligence than extraction method, and it is also more difficult and complicated. Several commonly used abstraction methods are: the model method, the term rewrite method, the event relation method, and the concept stage method.	Text summarization is the process of condensing a source text while pre-serving its information content and maintaining readability. The main difference between automatic and human-based text summarization is that humans can cap-ture and convey subtle themes that permeate documents, whereas automatic ap-proaches have a large difficulty to do the same. Nonetheless, as the amount of in-formation available in electronic format continues to grow, research into automatic text summarization has taken on renewed interest. 	One of the major new forums for presenting the latest techniques and comparing how well a text summarization approach works is in the Text Summa-rization Track of the Text Analysis Conference sponsored yearly by the National Institute of Standards and Technology (NIST) which provides problem defini-tions and a ground truth test data set to apply techniques to and compare results (http://www.nist.gov/tac) .		In most cases passage and sentence similarity analysis is performed along with computing a significance value for each.  Similar passages can be condensed to one extract.  In particular a sentence or passage that is closest to the centroid of a cluster of similar sentences/passages is a candidate to be selected for the sum-mary.  When going across multiple documents and additional rule such as sen-tences that are most similar to the item and most dissimilar to sentences already selected for the summary also help in selecting the correct sentence (Goldstein et al. 2000). The more complex approaches will attempt to understand the text to a deeper level than statistical.  They will use syntactic parsers and semantic analysis.  There are many dictionaries/thesaurus that can help support this process by look-ing up words of interest.	Some examples are WordNet (started in 1985), which has parts of speech and limited semantic network relationships.  ConceptNet from MIT Media Labs tries to merge more of a common sense understanding of ideas with a natural language processing approach.  FrameNet from Berkley contains more in depth linguistic knowledge that can be used in understanding text.7.4 Collaborative FilteringCollaborative filtering is another mechanism to help users in determining how to find information.  The technique is based upon other users who have simi-lar needs and what information path they followed to answer their question.  It al-so includes feedback from other users as well as what actions they took.  This is sometimes referred to as collective intelligence where enough samples can be used in determining a consensus decision.In collaborative filtering the system predicts what a user is interested in based upon historical data it has collected on other users.  When enough different previous user reactions are used the prediction accuracy improves.  Thus a com-parison is made of the current state and attributes of the current user with a similar state and attributes of previous users to predict the most likely next information item will be of interest. This often referred to as passive filtering. An alternative approach looks at specific entities and looks for users in-terested in those entities and other entities.  In this case users share their evalua-tion on specific entities that can be used in the estimation process.  It then predicts if a user is interested in specific entities – what other entities they are likely also to be interested in.  For example if a user indicates a wine they are interested in, the system can predict other wines they may be interested in based upon previous us-ers interested in that wine.  This is referred to as active filtering.  The active filter-ing can be on specific web pages (e.g., user ranking of them) or of a specific item – such as the wine example.One approach is memory based reasoning where the system tries to re-cognize examples from the past that equate to the current information need. The technical approach is divided into two major steps.  The first is a prediction of what might be of value to the user who has the information need.  The second is generation of a recommendation list for the user.  In the memory based approach the system looks at all of the other previous users of the system to determine users similar to the current user.  Then based upon that set of users and the current state and attributes the system predicts the top n likely items of interest.  This is also called nearest neighbor or user based collaborative filtering. User based correla-tion UBCF uses Pearson correlation item based collaborative filtering uses adjust-ed cosine similarity  http://delab.csd.auth.gr/~apostol/pubs/webmine2006.pdf.This is not related to search but more an augmentation to search or just brows-ing.  A user once they start looking at an item could either go back to hit list to see next item or the system could recommend where to go to next (navigation aide). The concept is collecting usage patterns by users of the system can recommend for new users what information is of value.  An earlier example already discussed was the page rank formula that suggested that the importance of a web page is based upon how much other pages (i.e., other users) link to that page.There are two approaches to collecting collaborative filtering data:  user centric and data centric. User centric asks users to tell the system what they like.   The system then looks across large number of users for data they entered and when new user is interested in one thing the system can suggest additional answers based upon the model it has built for similar users.  Data centric takes each entity as a point and then sees what else users are interested in that are at that entity or page.  Thus the system can keep track of where users go next given a page.  Or the system can track what else a user buys when they buy one item.  Based upon an-other user reaching that page/item the system can recommend where to go next.  It is based upon either users wanting to share information or passive filtering where the users allow the system to keep track of their clicks.7.5 Multimedia Presentation	One of the challenges in working with multimedia is how to create an in-terface to enter searches and how to display the results.  The methodologies vary based upon the modality of the multimedia as discussed in Chapter 1.  The user expects to have a combination of structured search and multimedia search in the basic search capability.  The structured search is against the citation metadata that describes the multimedia item as a minimum.  For example, the user expects to be able to include in his searches constraints on the hit list by date ranges, file names and source information when its available (e.g., what TV source that video in-dexing is from).  Given the complexity of multimedia search historically systems did not provide direct search of the multimedia.  Instead they indexed the text around, the file names associated with multimedia objects and when the user en-tered a textual query that was the index that was searched.  This section focuses on when the actual multimedia is searched.Following the format of Chapter 4 on indexing multimedia by addressing each modality separately, the presentations will be addressed in a similar order.7.5.1  Audio Presentation	Chapter 4 discussed the indexing techniques applied against audio sources.  In most cases what is of interest is what is being spoken in audio sources (i.e., transcribing the spoken words into text).  But in addition there is other metadata that can be found such as speaker identification and other special identi-fied sounds can be recognized (e.g., music, explosions).  But typically the special identified sounds need to be specified before the audio is indexed to be included in the index of the audio sources.	When the speech in the audio is being searched (the most common case) the speech is being transcribed into text typically in Unicode.  The text is then in-dexed using the text indexing algorithms discussed for normal textual items.  Since the index is a textual index the user interface can also be textual.  That is the user enters a textual search and it is run against the index.  The interface for search creation is a standard textual user interface.  For the other identified sounds or for locating a specific person leveraging off speaker identification, the interface is mainly textual.  The one exception is when a user is searching for music.  In that case the interface can allow the user to play some music and the system will take that audio input and search on it.  In that case the user will inform the system they are ready to search for music and then immediately play the music to be searched.  The interactive interface is more standard that the user pointing to previously stored audio file to use as the query. Although that approach would also be techni-cally feasible if desired.	The place where the interfaces begin to take on special characteristics is on display of the search results.  Since there is a direct correlation to the tran-scribed text and the original audio, both are used in the results GUI.  The results can be ordered by ranking and could even be displayed using clustering techniques because the index is textual.  That is for the case of transcribed text from Automat-ic Speech Recognition (ASR) indexing.  If phonetic indexing is used then only ranking is available because there are no semantic units (words) that could be clustered.  The ranking is modified to be based upon the phonetic search.  In the case of ASR, when a linear hit list is displayed, the system can create “snippets” of the transcribed text to be included in the hit list. When phonetic search (or spe-cial sound search) is used, the system can only provide the file name of the origi-nal source and an offset into it.  Thus the Latter ASR approach allows contextual information of the transcribed words around the hit term to be used by the user to understand the context of the search hits.	The most significant change in the GUI is when a user selects a specific item to be opened for detailed review.  In the case of phonetic search when a spec-ific item is opened the system knows the locations of all the found search terms.  Thus the system can allow a user to jump from one term to another and play the audio at that point.  Actually start playing the audio slightly before the hit point.  When the textual audio has been transcribed then there is a complete textual item that can be displayed.  But the textual item is linked to the original audio source.  In this case when the textual item is displayed there is also an associated multime-dia audio player that is synchronized with the textual item.  All of the search terms in the textual item can be highlighted to allow the user to quickly focus on where in the transcription the hit words were found.  The user then has both the first op-tion of looking at the context of each hit location and validating if that might be the information they are looking for. For example the words “plane” and “plain”  are phonetically identical but the other words in the context will disambiguate if an aeroplane, a woodworking tool or descriptive adjective is being spoken.  Since the speech to text process is errorful (at best in the 90% accuracy but often more in the 75-80% accuracy) the user will in many cases need to play the original source to be sure they are getting the correct information.  This is accomplished by keep-ing the offset of every transcribed word to the location in the audio for that word.  When the user clicks on the highlighted search term in the text (or any word in the transcribed text) the system can start to play the audio just before when that word was spoken.  At that point the user can use the playback controls of the multime-dia audio player to control the playback (e.g., pause, restart, speed up playback, slow down playback, etc).  Since there is a direct linkage between the transcribed text and the audio, as the user is playing the audio the system can highlight each transcribed word as it’s being played (highlighting the text is synchronized with the audio playback).  For places where the textual word was not recognized or there is no text in the audio, the highlighting freezes in the transcribed text until the next transcribed term is played.  An example of a system that does that is the Broadcast Monitoring System from BBN.  An example of the interface is shown in figure ????.  Figure ????When speaker identification is also applied then quite often the textual interface will partition the words spoken into which speaker (or indicating an unknown speaker) is saying the words.  When you are using a textual item as you original input there are many other syntactical and formatting cues in the textual item on what is being described and is used by the user to find information of value.  In audio transcription those cues are not in the audio but have to be, where possible, extrapolated from the audio – which means there will be errors in those cues.  The most significant ones are sentence boundaries and capitalization.  Capitalization is useful to help the user identify if a person is being discussed or just a noun or other descriptive word (e.g., “Bush” versus “bush”).   Sentence boundaries help in understanding breaks that separate the context from one idea to the next idea in the next sentence.  Systems attempt to identify sentence boundaries in audio by look-ing for pauses that often come as a person switches from one sentence to another.  Capitalization is much harder but for example when letters are spoken versus words it is typically an acronym for an organization and thus should be capital-ized.  Also in the original training of the speech to text system the training data is appropriately capitalized and that capitalization carries over to the dictionary asso-ciated with the ASR process.  So for example even if “ASR” was not part of the training data the system would capitalize it.  In addition a proper name like Italy would be transcribed with a capital letter.	Many systems are starting to cascade additional processing capabilities on the transcribed text.  In particular machine translation has been applied to the transcribed text to change from a news program in a language a user does not un-derstand to a news program in the user’s native language.  This process introduces an third “associatively” linked item.   There is the original audio along with the multimedia player for it.  There is the transcribed text along with links from each word in the transcribed text to the original audio.  There is now a machine transla-tion of the transcribed text that is produced (directly associated) with the tran-scribed text but is also associatively linked to the original audio.  Translation is typically not done on a word by word basis but is applied to a sentence or a phrase.  The translated text is not a positional word for word correlation to the original transcribed text.  Thus the relative time links in the translated text to the audio are not to the word level but the start of each translated sentence/phrase.  Thus all of the words in a translated segment will point to the same audio position-ing start point in the audio file at just before the beginning of the translated seg-ment.  If during playback of the audio the translated text is synchronized and high-lighted with the audio playback, it will be more a step function on highlighting the text where a phrase will be statically highlighted until the first word in the next phrase is reached.  Figure ???? from the BMS system shows an example of how a user interface could like displaying all three; audio (or in their case video from which audio is transcribed, the transcription and the machine translation.Figure ?????	The translated text is also searchable but if the transcribed text is 85-95% accurate the translated text will be significantly less than that based upon the im-pact of the mis-recognized words in the transcription process.  But errors in the transcription process and the translation process may not have a direct correlation to inaccuracy in the search process.   Most of the words in text are not ever searched on.  What is typically searched on are names and other descriptive words that tend to be long in nature.  The speech to text process tends to be more accu-rate for longer words with more of the errors being on shorter words.  If the longer words are transcribed correctly they will have a stronger likelihood to be translat-ed correctly.   The initial goal of the speech to text and machine translation pro-cess is to create the index to the original multimedia object not as a replacement for it.  Thus the cascading errors in the process may not have as significant effect on overall user search performance as indicated by the accuracy of each step.  That being said the ultimate vision would be if both processes could be accurate enough to be directly usable in addition to the multimedia object.7.5.2  Image Item Presentation	The presentation for search of audio demonstrated close parallel to textu-al presentations because the audio was converted to text and could leverage off the textual options.  When considering image search those options do not exist to the same degree.  As noted in the introduction to this section historically the multime-dia image was (and is) not indexed.  Instead the text associated with the image is indexed.  This allows for a simple user interface that is textual in nature.  The user can type in a search statement on what they want to see and the system will search the indexed text associated with images and return in a ranked list the possible hits.  Since the user goal is to find an image typically a thumbnail of the image is also presented to the user in the sequential ranked hit list.  Example ????  is from a simple GOOGLE search which also shows additional filtering options that can be used to get more precise results. Figure ????  Google Image search results pageGoogle search is primarily focused on text associated with the image.  But GOOGLE does allow filtering on specific characteristics of an image or categori-zations that they have preprocessed the image to belong to.  Thus you can filter on image size, colors or if the image is of a face, clip art, photo or line drawing.	The goal in the information retrieval system is to index the semantics of the image and not just the text associated with the image.  Thus the search inter-face must allow a user to identify an image to be searched to find other images like it.  In addition to identifying an image another critical aspect of defining the search is to be able to select a subset of the displayed image as the search image versus the total image. That can significantly increase the accuracy of the search and save the user significant time.  Without the capability to “rubber band” define a subset of an image the user would need to use a multimedia edit tool to “crop” the original image to define a new image of exactly what they want to search for.	In addition to the image to be used as the query, the user should also be able to adjust parameters on how important characteristics of the image are to again increase the precision.  For example the user should be able to specify how import color is to the search.  If I am searching for a car and the image I have is for a red car, it would be useful to indicate to the system that I am more interested in the shape of the object – a car – then finding images that have a lot of red in them.  The specific attributes that the user can tune will be specific to what the at-tributes from the image are indexed.  Since the search process of images will be significantly more likely to return erroneous images, another option the user typi-cally has is to specify how accurate the search should be – if they want any hit or those with a high precision.  This is useful in reducing the size of the hit file and the overhead of creating the display page (and the time to create the hit display page).  Figure ???? is from the PixLogic system and shows one approach to the search user interface.  It shows how to “rubber band” around just the burning car to make the image search more focused on it.  It has thresholds for color, the im-portance of shape, foreground, background and color as well as a setting for con-fidence level.  On the left hand side it’s possible to specify multiple images and include Boolean logic between them.  When an “and” is specified between two images in a search it means that the indexable attributes from both images are treated as if they were all from a single image and those attributes are used in the search process against the attributes associated with other images.  Unless the con-fidence level is set to high the system will return images that have subsets of the attributes.  Thus it is not a “strict” AND in the Boolean sense where a returned hit must have all of the specifications in it.  This can be seen clearer in Figure ???? where search for text within the image is also added to the query frame. Figure 7.????   Image Search interface Imbedded within an image there are also additional artifacts that by them selves can be segmented and made searchable.  The segmentation (just identifying that portion of the image with the artifact) will increase the accuracy and processing of the artifact.  The best example is text that is imbedded within the image.  The text within an image should not be searched using the same algorithms that objects within an image are searched for.  Instead the text within an image can easily be identified and then it should be searchable where the user enters a textual query.   The technologies to make the text searchable are discussed in Chapter 4 (e.g., op-tical character reading, text image segmentation).  But the presentation interface should allow the user to enter text to be found as text within the image.  That should be part of an overall search statement that should allow both text in images as well as objects to be part of the query specification.  An example of that more complex search statement is shown for PixLogic in figure ?????.  the search is looking for a car accident as an image and where there is text on the image with the word accident (e.g., subtitled news displays). Another artifact that can be separately identified in an image with focused in-dexing is logos.  They are very useful in identifying some aspects of the topic within an image.  In this case the user should be able to present a logo looking for images that have that logo.  In addition to artifacts such as they discussed there are other objects that might have an importance to a set of users that focused attempts of uniquely identifying them within an image may significantly improve the accu-racy of a users search.  For example identifying maps within an image or where they are the image or wiring diagrams (these have a lot in common in terms of im-age characteristics) could be useful allowing a user to specify they want to search to find that specific generic object rather than presenting an image an finding other image of a map and finding other images like it.  This would again have a textual interface where the list of these special objects would be available that a user could select from as part of their query.	The search results from an image search is a ranked list of images and as mentioned the standard presentation is by showing the title (e.g., file name) of the image along with a thumbnail of the image.  When a user selects a particular im-age to display from the hit list the image should come up in its own display win-dow.  Some non-critical but useful display operations associated with display of an image is to be able to “zoom” in on a portion of the image to see it in more de-tail.  But the one function that is needed when displaying an image is to select that image (or a rubber banded portion of the image) to be included on a follow-on search.  	Although a sequential list of the images along with the name associated with the image is one display approach.  Since the user is interested in looking for images a more dense display where only the thumbnails are displayed getting as many as possible on the display page.  This will allow the user to quickly scan the thumbnail search results for the images they are looking for. The human visual system can process a lot of image very quickly and is optimized to do that looking for the real “information need” of the user.   A characteristic of this display that is useful is the ability to define the size of the thumbnails - making them all larger will make them easier to recognize but have fewer per display page while smaller has the opposite effect.  Figure ????? shows how multiple search hits can be dis-played more effectively than a sequential display. Figure 7.???? Display of maximum number of thumbnails.7.5.3 Video Presentation	As mentioned in Chapter 4 on indexing, a video is really a combination of both an audio track and an image track.  When looking to the search entry and results displays for a video, all of the discussions and the results presentations dis-cussed in the last two sections are applicable and both could be combined in the same query.  In addition to those there are additional tracks of information that are correlated to relative time within the video.   In particular there is closed caption-ing and teletext that could also be available an indexed.   They would add addi-tional text entry aspects for the search.  Thus the query generation display would have areas for search entry for images to be searched as frames within the video as well as textual entry to search the audio transcription, machine translation (if available), text on the images and closed captioning/teletext.	The search results options would be similar to image search except the thumbnail would be associated with the first frame in the video that satisfied the image search.  The thumbnail would also be point to a video that would be opened in a video player when selected.   If only an image search was entered, then the thumbnail only display would be optimum displaying as many different hit videos as possible on the display screen.  It there was an associated text aspect to the search then snippets could also be displayed to help indicate the context of some of the text that satisfied the query.	When considering textual searches if the user opens the item, the full text can be displayed with the hit terms highlighted.   When an image is part of the search a different strategy is needed in generating the display of a particular video item.  When an item is opened up then what is displayed would be the thumbnails of all of the scenes within that video that satisfied the query.  Thus for video there is a two tiered display process.  The thumbnail representing the hit of a video item would be the thumbnail of the scene that has the highest weight for that item.  But when the item is opened the thumbnails for all of the hit scenes would be dis-played along with an indication of the confidence level of that scene matching the search image.  The risk associated with this display hierarchy is that the highest ranked scene that is used in the hit list might be a non-relevant frame.  A lower weighted frame could satisfy the user’s information need but the user would never know and skip over that item.  This is similar to when a user looking at a textual snippet thinks the item is not relevant when there could be lower weighted rele-vant information in an item.  Clicking on any of the thumbnails representing hits would start the video item from playing at that offset within the item. 	Since the multimedia item is now a video there is an additional search op-tion available where the search query is a video clip.  The concept is that the user is trying to find where a video clip or subsets of a search video clip are found within the video item database.  This is a more complex search because it is pos-sible that part of the search video is found in the video item as well as the entire search video clip.  When a specific video item is opened the video search clip needs to be mapped to the locations within the video item that satisfy some or the entire video clip.  One approach to displaying that information is to have a linear line representing the search video clip and another line representing the video item and showing mapping to segments within the video item that satisfy some or all of the video clip.  Figure ????? shows one example of how that information could be presented.7.6  Human Perception and PresentationThe primary focus on Information Retrieval Systems has been in the areas of indexing, searching and clustering versus information display.  This has been due to the inability of technology to provide the technical platforms needed for sophis-ticated display, academic’s focusing on the more interesting algorithmic based search aspects of information retrieval, and the multi-disciplinary nature of the human-computer interface (HCI).  The core technologies needed to address so-phisticated information visualization have matured, supporting productive re-search and implementation into commercial products.  The commercial demand for these technologies is growing with availability of the “information highway.”  System designers need to treat the display of data as visual computing instead of treating the monitor as a replica of paper.  Functions that are available with elec-tronic display and visualization of data that were not previously provided are (Brown-96): modify representations of data and information or the display condition (e.g., changing color scales)use the same representation while showing changes in data (e.g., moving between clusters of items showing new linkages)animate the display to show changes in space and timeenable interactive input from the user to allow dynamic movement be-tween information spaces and allow the user to modify data presentation to optimize personal preferences for understanding the data.Create hyperlinks under user control to establish relationships between data  If information retrieval had achieved development of the perfect search algorithm providing close to one hundred per cent precision and recall, the need for advances in information visualization would not be so great.  But reality has demonstrated in TREC and other information forums that advancements are not even close to achieving this goal.  Thus, any technique that can reduce the user overhead of finding the needed information will supplement algorithmic achieve-ments in finding potential relevant items.  Information Visualization addresses how the results of a search may be optimally displayed to the users to facilitate their understanding of what the search has provided and their selection of most likely items of interest to read.  Visual displays can consolidate the search results into a form easily processed by the user’s cognitive abilities, but in general they do not answer the specific retrieval needs of the user other than suggesting data-base coverage of the concept and related concepts.	The theoretical disciplines of cognitive engineering and perception provide a theoretical base for information visualization. Cognitive engineering de-rives design principles for visualization techniques from what we know about the neural processes involved with attention, memory, imagery and information pro-cessing of the human visual system.  By 1989 research had determined that mental depiction plays a role in cognition that is different from mental description.  Thus, the visual representation of an item plays as important a role as its symbolic defi-nition in cognition.  	Cognitive engineering results can be applied to methods of reviewing the concepts contained in items selected by search of an information system. Visuali-zation can be divided into two broad classes:  link visualization and attribute (con-cept) visualization.  Link visualization displays relationships among items. At-tribute visualization reveals content relationships across large numbers of items. Related to attribute visualization is the capability to provide visual cues on how search terms affected the search results.  This assists a user in determining changes required to search statements that will return more relevant items.7.6.1  Introduction to Information Visualization	The beginnings of the theory of visualization began over 2400 years ago.  The philosopher Plato discerned that we perceive objects through the senses, using the mind.  Our perception of the real world is a translation from physical energy from our environment into encoded neural signals.  The mind is continually inter-preting and categorizing our perception of our surroundings.  Use of a computer is another source of input to the mind’s processing functions.  Text-only interfaces reduce the complexity of the interface but also restrict use of the more powerful information processing functions the mind has developed since birth.Information visualization is a relatively new discipline growing out of the debates in the 1970s on the way the brain processes and uses mental images.  It required significant advancements in technology and information retrieval tech-niques to become a possibility.  One of the earliest researchers in information vis-ualization was Doyle, who in 1962 discussed the concept of “semantic road maps” that could provide a user a view of the whole database (Doyle-62).  The road maps show the items that are related to a specific semantic theme.  The user could use this view to focus his query on a specific semantic portion of the database.  The concept was extended in the late 1960s, emphasizing a spatial organization that maps to the information in the database (Miller-68).  Sammon implemented a non-linear mapping algorithm that could reveal document associations providing the information required to create a road map or spatial organization (Sammons-69).   In the 1990s technical advancements along with exponential growth of available information moved the discipline into practical research and commer-cialization.  Information visualization techniques have the potential to significant-ly enhance the user’s ability to minimize resources expended to locate needed in-formation.  The way users interact with computers changed with the introduction of user interfaces based upon Windows, Icons, Menus, and Pointing devices (WIMPs).  Although movement in the right direction to provide a more natural human interface, the technologies still required humans to perform activities opti-mized for the computer to understand.  A better approach was stated by Donald A. Norman (Rose-96):… people are required to conform to technology.  It is time to reverse this trend, time to make technology conform to people Norman stresses that to optimize the user’s ability to find information, the fo-cus should be on understanding the aspects of the user’s interface and processing of information which then can be migrated to a computer interface (Norman-90).  	Although using text to present an overview of a significant amount of in-formation makes it difficult for the user to understand the information, it is essen-tial in presenting the details.  In information retrieval, the process of getting to the relevant details starts with filtering many items via a search process. The result of this process is still a large number of potentially relevant items. In most systems the results of the search are presented as a textual list of each item perhaps ordered by rank.  The user has to read all of the pages of lists of the items to see what is in the Hit list.  Understanding the human cognitive process associated with visual data suggests alternative ways of presenting and manipulating information to fo-cus on the likely relevant items. There are many areas that information visualiza-tion and presentation can help the user:a. 	reduce the amount of time to understand the results of a search and likely clusters of relevant informationb.   yield information that comes from the relationships between items       versus treating each item as independentc.   perform simple actions that produce sophisticated information search     functions   A study was performed by Fox et al. using interviews and user task analysis on professionals in human factors engineering, library science, and computer science to determine the requirements to optimize their work with documents (Fox-93a).  Once past the initial requirement for easy access from their office, the researchers’ primary objective was the capability to locate and explore patterns in document databases.  They wanted visual representations of the patterns and items of inter-est.  There was a consistent theme that the tools should allow the users to view and search documents with the system sensitive to their view of the information space.  The users wanted to be able to focus on particular areas of their interest (not ge-neric system interest definitions) and then easily see new topical areas of potential interest to investigate.  They sought an interface that permits easy identification of trends, interest in various topics and newly emerging topics.  Representing infor-mation in a visual mode allows for cognitive parallel processing of multiple facts and data relationships satisfying many of these requirements. The exponential growth in available information produces large Hit files from most searches. To understand issues with the search statement and retrieved items, the user has to review a significant number of status screens. Even with the review, it is hard to generalize if the search can be improved.  Information visuali-zation provides an intuitive interface to the user to aggregate the results of the search into a display that provides a high-level summary and facilitates focusing on likely centers of relevant items.  The query logically extracts a virtual work-space (information space) of potential relevant items which can be viewed and manipulated by the user.  By representing the aggregate semantics of the work-space, relationships between items become visible.  It is impossible for the user to perceive these relationships by viewing the items individually. The aggregate presentation allows the user to manipulate the aggregates to refine the items in the workspace.  For example, if the workspace is represented by a set of named clus-ters (name based upon major semantic content), the user may select a set of clus-ters that defines the next iteration of the search.An alternative use of aggregates is to correlate the search terms with items retrieved.  Inspecting relevant and non-relevant items in a form that high-lights the effect of the expanded search terms provides insights on what terms were the major causes for the results.  A user may have thought a particular term was very important.  A visual display could show that the term in fact had a mini-mal effect on the item selection process, suggesting a need to substitute other search terms.Using a textual display on the results of a search provides no mechanism to display inter-relationships between items.  For example, if the user is interested in the development of a polio vaccine, there is no way for a textual listing of found items to show “date” and “researcher” relationships based upon published items.  The textual summary list of the Hit file can only be sorted via one attribute, typi-cally relevance rank.  	Aspects of human cognition are the technical basis for understanding the details of information visualization systems. Many techniques are being developed heuristically with the correlation to human cognition and perception analyzed after the techniques are in test. The commercial pressures to provide visualization in de-livered systems places the creativity under the intuitive concepts of the developer.   7.5.2  Cognition and Perception	The user-machine interface has primarily focused on a paradigm of a typewriter.  As computers displays became ubiquitous, man-machine interfaces focused on treating the display as an extension of paper with the focus on con-sistency of operations.  The advent of WIMP interfaces and simultaneous parallel tasks in the user work environment expanded the complexity of the interface to manipulate the multiple tasks.  The evolution of the interface focused on how to represent to the user what is taking place in the computer environment.  The ad-vancements in computer technology, information sciences and understanding hu-man information processing are providing the basis for extending the human com-puter interface to improve the information flow, thus reducing wasted user overhead in locating needed information. Although the major focus is on enhanced visualization of information, other senses are also being looked at for future inter-faces. The audio sense has always been part of simple alerts in computers. Illegal inputs are usually associated with a beep, and more recently users have a spectrum of audio sounds to associate with everything from start-up to shut down. The sounds are now being replaced by speech in both input and output interfaces.  Still in the research arena is the value of using audio to encapsulate information (e.g., higher pitch as you move through an information space plus increased relevance).  The tactile (touch) sense is being addressed in the experiments using Virtual Real-ty (VR).  For example,  VR is used as a training environment for areas such as medical procedures where tactile feedback plays an increasing role. Olfactory and taste are two areas where practical use for information processing or computer in-terfaces in general has yet to be identified.  For Information Retrieval Systems, the primary area of interest is in information visualization. 7.5.2.1  Background	A significant portion of the brain is devoted to vision and supports the maximum information transfer function from the environment to a human being. The center of debates in the 1970s was whether vision should be considered data collection or also has aspects of information processing.  In 1969 Arnheim ques-tioned the then current psychological division of cognitive operations of percep-tion and thinking as separate processes (Arnheim-69). Until then perception was considered a data collection task and thinking as a higher level function using the data.  He contended that visual perception includes the process of understanding the information, providing an ongoing feedback mechanism between the percep-tion and thinking.  He further expanded his views arguing that treating perception and thinking as separate functions treats the mind as a serial automata (Arnheim-86).  Under this paradigm, the two mental functions exclude each other, with per-ception dealing with individual instances versus generalizations.  Visualization is the transformation of information into a visual form which enables the user to ob-serve and understand the information.  This concept can be extended where the visual images provide a fundamentally different way to understand information that treats the visual input not as discrete facts but as an understanding process.  The Gestalt psychologists postulate that the mind follows a set of rules to combine the input stimuli to a mental representation that differs from the sum of the indi-vidual inputs (Rock-90):Proximity - nearby figures are grouped togetherSimilarity - similar figures are grouped togetherContinuity - figures are interpreted as smooth continuous patterns ratherthan discontinuous concatenations of shapes (e.g., a circle withits diameter drawn is perceived as two continuous shapes, a cir-cle and a line, versus two half circles concatenated together)Closure - gaps within a figure are filled in to create a whole (e.g., using dashed lines to represent a square does not prevent understand-ing it as a square)Connectedness - uniform and linked spots, lines or areas are perceived as a single unitShifting the information processing load from slower cognitive processes to faster perceptual systems significantly improves the information-carrying inter-faces between humans and computers (Card-96).  There are many ways to present information in the visual space.  An understanding of the way the cognitive pro-cesses work provides insights for the decisions on which of the presentations will maximize the information passing and understanding. There is not a single correct answer on the best way to present information. 7.5.2.2  Aspects of the Visualization Process	One of the first-level cognitive processes is preattention, that is, taking the significant visual information from the photoreceptors and forming primitives.   Primitives are part of the preconscious processes that consist of involuntary lower order information processing (Friedhoff-89).  An example of this is the ease with which our visual systems detect borders between changes in orientation of the same object.  In Figure 8.1 the visual system detects the difference in orientations between the left and middle portion of the figure and determines the logical border between them.   An example of using the conscious processing capabilities of the brain is the detection of the different shaped objects and the border between them shown between the left side and middle of the Figure 8.1.  The reader can likely detect the differences in the time it takes to visualize the two different boundaries.	⇑    	⇑  ⇑    ⇑	⇐  ⇐   ⇐        ⇐  ⇐     ⊂⊂⊂ ⊂ ⊂   ⊂⊂     ⇑  ⇑    ⇑  ⇑     ⇑        ⇐ ⇐    ⇐ ⇐   ⇐  ⇐   ⇐ ⇐  ⊂    ⊂⊂⊂  ⊂  ⊂	 ⇑ ⇑  ⇑  ⇑   ⇑   ⇐ ⇐⇐⇐     ⇐    ⇐  ⇐  ⇐ ⇐   ⊂⊂  ⊂  ⊂  ⊂⊂       ⇑  ⇑ ⇑ ⇑    ⇑  ⇑ ⇑     ⇐  ⇐ ⇐   ⇐  ⇐           ⇐⇐   ⊂ ⊂ ⊂ ⊂ ⊂ ⊂   ⇑   ⇑  ⇑  ⇑  ⇑  ⇑        ⇑    ⇐⇐  ⇐   ⇐   ⇐⇐⇐      ⇐   ⊂ ⊂⊂⊂   ⊂⊂Figure 7.1  Preattentive Detection MechanismThis suggests that if information semantics are placed in orientations, the mind’s clustering aggregate function enables detection of groupings easier than using different objects (assuming the orientations are significant).  This approach makes maximum use of the feature detectors in the retina.The preattentive process can detect the boundaries between orientation groups of the same object. A harder process is to identify the equivalence of ro-tated objects.  For example, a rotated square requires more effort to recognize it as a square.  As we migrate into characters, the problem of identification of the char-acter is affected by rotating the character in a direction not normally encountered.  It is easier to detect the symmetry when the axis is vertical.  Figure 8.2 demon-strates these effects. R  ∃  ∀  L      R E  Α  LFigure 7.2  Rotating a Square and Reversing Letters in “REAL” 	Another visual factor is the optical illusion that makes a light object on a dark background to appear larger than if the item is dark and the background is light.  Making use of this factor suggests that a visual display of small objects should use bright colors.  An even more complex area is the use of colors.  Colors have many attributes that can be modified such as hue, saturation and lightness.  Hue is the physiological attribute of color sensation.  Saturation is the degree to which a hue is different from a gray line with the same lightness, while lightness is the sensation of the amount of white or black.  Complementary colors are two colors that form white or gray when combined (red/green, yellow/blue).  Color is one of the most frequently used visualization techniques to organize, classify, and enhance features (Thorell-90).  Humans have an innate attraction to the primary colors (red, blue, green and yellow), and their retention of images associated with these colors is longer.  But colors also affect emotion, and some people have strong aversion to certain colors. The negative side of use of colors is that some people are color blind to some or many colors.  Thus any display that uses colors should have other options available.	Depth, like color, is frequently used for representing visual information.  Classified as monocular cues, changes in shading, blurring (proportional to dis-tance), perspective, motion, stereoscopic vision, occlusion and texture depict depth.  Most of the cues are affected more by lightness than contrast.  Thus, choice of colors that maximizes brightness in contrast to the background can assist in pre-senting depth as a mechanism for representing information.  Depth has the ad-vantage that depth/size recognition are learned early in life and used all of the time. Gibson and Walk showed that six-month-old children already understand depth suggesting that depth may be an innate concept (Gibson-60).  The cognitive processes are well developed, and the use of this information in classifying objects is ubiquitous to daily life. The visual information processing system is attuned to processing information using depth and correlating it to real world paradigms.	Another higher level processing technique is the use of configural aspects of a display (Rose-95).  A configural effect occurs when arrangements of objects are presented to the user allowing for easy recognition of a high-level abstract condition.    Configural clues substitute a lower level visual process for a higher level one that requires more concentration (see preattentive above).  These clues are frequently used to detect changes from a normal operating environment such as in monitoring an operational system.  An example is shown in Figure 8.3 where the sides of a regular polygon (e.g., a square in this example) are modified.  The visual processing system quickly detects deviations from normally equally sized objects.	Another visual cue that can be used is spatial frequency.  The human vis-ual and cognitive system tends towards order and builds a coherent visual image whenever possible.  The multiple spatial channel theory proposes that a complex image is constructed from the external inputs, not received as a single image.  The final image is constructed from multiple receptors that detect changes Figure 7.3  Distortions of a Regular Polygon 	in spatial frequency, orientation, contrast, and spatial phase.  Spatial frequency is an acuity measure relative to regular light-dark changes that are in the visual field or similar channels.  A cycle is one complete light-dark change.  The spatial frequency is the number of cycles per one degree of visual field.  Our visual sys-tems are less sensitive to spatial frequencies of about 5-6 cycles per degree of vis-ual field (NOTE: one degree of visual field is approximately the viewing angle subtended by the width of a finger at arms length). Other animals have significant-ly more sensitive systems that allow them to detect outlines of camouflaged prey not detected by humans until we focus on the area.  Associated with not pro-cessing the higher spatial frequencies is a reduction in the cognitive processing time, allowing animals (e.g. cats) to react faster to motion. When looking at a dis-tinct, well defined image versus a blurred image, our visual system will detect mo-tion/changes in the distinct image easier than the blurred image.  If motion is be-ing used as a way of aggregating and displaying information, certain spatial frequencies facilitate extraction of patterns of interest.  Dr. Mary Kaiser of NASA-AMES is experimenting with perceptually derived displays for aircraft.  She is in-terested in applying the human vision filters such as limits of spatial and temporal resolution, mechanisms of stereopsis, and attentional focus to aircraft (Kaiser-96).	The human sensory systems learn from usage.  In deciding upon visual information techniques, parallels need to be made between what is being used to represent information and encountering those techniques in the real world envi-ronment.  The human system is adept at working with horizontal and vertical ref-erences.  They are easily detected and processed.  Using other orientations re-quires additional cognitive processes to understand the changes from the expected inputs.  The typical color environment is subdued without large areas of bright colors.  Thus using an analogous situation, bright colors represent items to be fo-cused on correlating to normal processing (i.e., noticing brightly colored flowers in a garden).  Another example of taking advantage of sensory information that the brain is use to processing is terrain and depth information.  Using a graphical rep-resentation that uses depth of rectangular objects to represent information is an image that the visual system is used to processing.  Movement in that space is mo-re easily interpreted and understood by the cognitive processes than if, for exam-ple, a three-dimensional image of a sphere represented a visual information space. 	In using cognitive engineering in designing information visualization techniques, a hidden risk is that “understanding is in the eye of the beholder.”  The integration of the visual cues into an interpretation of what is being seen is also based upon the user’s background and context of the information.  The human mind uses the latest information to assist in interpreting new information.  If a par-ticular shape has been representing important information, the mind has a predis-position to interpret new inputs as the same shape.   For example, if users have been focusing on clusters of items, they may see clusters in a new presentation that do not exist.  This leads to the question of changing visualization presenta-tions to minimize legacy dispositions.  Another issue is that our past experiences can affect our interpretation of a graphic.  Users may interpret figures according to what is most common in their life experiences rather than what the designer in-tended. 7.6 Summary	Search algorithms are very important in information retrieval because they define the subset of the database that will be displayed to the user and the ranked order of what is most likely to be of interest.  Just as important to the user finding what they are looking for is how the information is presented.  In some cases the standard linear ranked list is sufficient for the user to locate a few items on a particular topic.  But when the user is looking for a more exhaustive set of in-formation or trying to understand the different subtopics of information on a par-ticular problem other presentation techniques such as clustering is the best meth-odology to help the user to navigate to the needed information.  Although the more sophisticated methods for information visualization require significant more computer resources, advances in hardware performance and software architecture are making their common use possible over the next few years.  The end result will be to marry the advances in searching with sophisticated results presentation allowing the user to leverage off their minds ability to absorb and assess patterns and specific information in more complex displays resulting on detection of their needed information faster and more comprehensive.  	Text summarization techniques also hold significant promise in eliminat-ing the redundancy of information within and especially between hits allowing the user to focus on the specific new facts rather than having to reread the same data over many times to detect one new piece of information.	Information presentation for multimedia searching is still in its infancy as is generally providing search of multimedia items.  But there is tremendous pres-sure on developing information retrieval technologies in this area as cell phones and other Personal Digital Assistants become common place.EXERCISES ;1.	What are the technical issues with providing clustering presentation with every search?  Is there some preprocessing approach that could make such a presentation more realistic/2.	In order to do timeline presentation what information is needed?  How would that information be determined?3.	Use the autosummarize capability in MS Word against a textual items you have.  How well did it work.  Search the Internet for other sites that allow you to submit text to be summarized and look at those results.  What seems to work and what are the inherent limits in text summariza-tion. 4.	 Expand the discussion on text summarization to when you are summa-rizing across multiple items.  What functions and capabilities are essen-tial in the display of the summarized information to assist the user in val-idating the results and feeling confident about its completeness?5.	What are the basic limitations and difficulties in a user generating a search and getting results back from an image or video image search?  What unique functions need to be provided to allow the user to validate the results of their search (map their search to each result returned) and to enhance the search to make it more precise?
4 IndexingAbstract:  Once the processing tokens and other metadata that is associated with items have been identified, the system then needs to construct the searchable index that will be used with the user’s queries to return results.  Before discussing the detailed alternatives, the concept of indexing is discussed along with what its ob-jective.  The automatic textual indexing techniques are discussed in the context of statistical, natural language and concept indexing approaches.  Statistical is the major technique used in current systems and the major algorithms used are dis-cussed (e.g., term frequency/inverse document frequency with item length normal-ization).  Finally the indexing techniques associated with multimedia items (audio, image and video items) are described. 4.1  What is Indexing	Chapter 3 focused on the initial processing (ingest) of an item.  It con-cluded with having identified the processing tokens that would be used to create the searchable index for the item.  Before the specific indexing techniques are dis-cussed it’s useful to understand what an index is and what its goal is.  One of the most critical aspects of an information retrieval system that determines its effectiveness is how it represents concepts (semantics) in items.  The transformation from the received item to the searchable data structure is called Indexing.  This process historically was manual but is now primarily automatic, creating the basis for search of items.  The index is what really defines an item more than its original content.  This is because the primary mechanism to get to an item is based upon search of the index.  If information is not in the index, then the user will never know the item exists.  For example, if a new term to describe a process is unique and found in one item, it’s possible that the stop algorithm process will eliminate the processing token before it gets to the indexing phase.  If a user searches for that unique word it will appear as if there are no items in the database that contain that term.  Indexing is the process of mapping from the contents of an item to the searchable structure used to find items.  If there are concepts in the item that are not reflected in the index, then a user will not find that item when searching for those concepts.  In addition to mapping the concepts to the searchable data structure, the automatic indexing process may attempt to as-sign a weight on how much that item discusses a particular concept.  This is used in the display phase for ranking the outputs, attempting to get the items more like-ly to be relevant higher in the hit list.  To better understand the indexing process a discussion of manual indexing process sheds some insights into the automatic in-dexing process.	Once the processing tokens have been identified they can be used to cre-ate the searchable index for the item.  The index that is created defines how well an information retrieval system will perform.  Users do not browse all of the items in the database unless there is Taxonomy and they follow the taxonomy tree.  But in a sense that is also a search where each level in the taxonomy can be defined by a search.  Since the users can only find items of interest by searching then if the semantics of what is important within an item is not reflected in the searchable in-dex, users will never find the item concerning that semantics.Before introducing the actual indexing methodologies, reviewing the his-tory of indexing puts into perspective the importance of indexing and how it evolved.  Through most of the 1980’s the goals of commercial Information Re-trieval Systems were constrained to facilitating the manual indexing paradigm.  In the 1990’s, exponential growth in computer processing capabilities with a continu-ing decrease in cost of computer systems has allowed Information Retrieval Sys-tems to implement previously theoretical functions, introducing a new information retrieval paradigm where the text of the item could be the index. In the 2000’s the technology had evolved where it was not only scalable to billions of items being indexed, but also other support technologies such as entity identification, duplicate removal and categorization can be used to enhance both precision and recall.  But there still there remains a place for manual indexing.4.1.1  History 	Indexing (originally called Cataloging) is the oldest technique for identifying the contents of items to assist in their retrieval.  The objective of cataloging is to give access points to a collection that are expected and most useful to the users of the information.  The basic information required on an item, what is the item and what it is about, has not changed over the centuries.  As early as the third-millennium, in Babylon, libraries of cuneiform tablets were arranged by sub-ject (Hyman-89). Up to the 19th Century there was little advancement in catalog-ing, only changes in the methods used to represent the basic information (Norris-69).  In the late 1800s subject indexing became hierarchical (e.g., Dewey Decimal System). In 1963 the Library of Congress initiated a study on the computerization of bibliographic surrogates.  From 1966 - 1968 the Library of Congress ran its MARC I pilot project.  MARC (MAchine Readable Cataloging) standardizes the structure, contents and coding of bibliographic records.  The system became operational in 1969 (Avram-75).  The earliest commercial cataloging system is DIALOG, which was developed by Lockheed Corporation in 1965 for NASA.  It became commercial in 1978 with three government files of indexes to technical publications.  By 1988, when it was sold to Knight-Ridder, DIALOG contained over 320 index databases used by over 91,000 subscribers in 86 countries (Harper-81).  	Indexing (cataloging), until recently, was accomplished by creating a bib-liographic citation in a structured file that references the original text.  These files contain citation information about the item, keywording the subject(s) of the item and, in some systems a constrained length free text field used for an abstract/summary. The indexing process is typically performed by professional indexers associated with library organizations.  Throughout the history of libraries, this has been the most important and most difficult processing step.  Most items are retrieved based upon what the item is about.  The user’s ability to find items on a particular subject is limited by the indexer creating index terms for that sub-ject. But libraries and library indexing has always assumed the availability of the library staff to act if needed as a human intermediary for users having problems in locating information.  Users looking for well-defined data (e.g., people by name and titles) have good success by themselves.  But when users are searching for topics they fail on 70% of single query requests and 45% of the time they never find the data they need.  But when the users consult with a librarian the failure rates drop to 10% (Nordlie-99.)  Thus library based indexing was never under significant pressure to invent user interfaces, support material and augmented search engines that would assure users could find the material they needed.  They could rely on human interaction to resolve the more complex information needs.	The initial introduction of computers to assist the cataloguing function did not change its basic operation of a human indexer determining those terms to assign to a particular item.  The standardization of data structures (e.g., MARC format) did allow sharing of the indexes between libraries.  It reduced the manual overhead associated with maintaining a card catalog.  By not having to make physical copies of the index card for every subject index term, it also encouraged inclusion of additional index terms.  Also it allowed for Boolean logic to further refine the user’s request (e.g., author and subject).  But the process still required the indexer to enter index terms that quite often were redundant with the words in the referenced item.  The user, instead of searching through physical cards in a card catalog, now performed a search on a computer and electronically displayed the card equivalents.  It allows the user to search on multiple index terms using Boolean logic allowing for more precise retrieval.	In the 1990s, the significant reduction in cost of processing power and memory in modern computers, along with access to the full text of an item from the publishing stages in electronic form, allowed use of the full text of an item as an alternative to the indexer-generated subject index.  The searchable availability of the text of items has changed the role of indexers and allowed introduction of new techniques to facilitate the user in locating information of interest.  The in-dexer is no longer required to enter index terms that are redundant with words in the text of an item.  The searcher is no longer presented a list of potential item of interest, but is additionally informed of the likelihood that each item satisfies his search goal.  Additional new automated tools such as entity identification, catego-rization have moved the processing closer to full automation, but manual indexing still holds value (e.g., Yahoo).4.1.2  Objectives	The objective of indexing has not changed from the earliest days of li-braries.  The index needs to represent the semantics of the item that is of potential interest to users.  If there are semantics (ideas) in the item that are not reflected in the index, users will not be able to find the item.  This is true from the days of using physical rooms to index, to adding index terms in card catalogs and making copies of the index cards to the automatic indexing algorithms discussed in this chapter. How indexing occurs and the roles played by machines and humans have changed with the evolution of Information Retrieval Systems.  Availability of the full text of the item in searchable form alters the manual indexing needs historical-ly used in determining guidelines for manual indexing.  In the new environment where all of the processing tokens in all items are indexed, all of the words within the item are potential index descriptors of the subject(s) of the item.  Chapter 3 discusses the ingest process that takes all possible words in an item and transforms them into processing tokens used in defining the searchable representation of an item.  In addition to determining the processing tokens, current systems have the ability to automatically weight the processing tokens based upon their potential importance in defining the concepts in the item and assigning items to locations in taxonomy that describes the overall subject of the information database. 	The first reaction of many people is to question the need for manual in-dexing at all, given that total document indexing is available for search.  If one can search on any of the words in a document why does one need to add additional in-dex terms?  Previously, indexing defined the source and major concepts of an item and provided a mechanism for standardization of index terms (i.e., use of a con-trolled vocabulary).  A controlled vocabulary is a finite set of index terms from which all index terms must be selected (the domain of the index).  In a manual indexing environment, the use of a controlled vocabulary makes the indexing process slower, but potentially simplifies the search process.  The extra processing time comes from the indexer trying to determine the appropriate index terms for concepts that are not specifically in the controlled vocabulary set. Controlled vocabularies aide the user in knowing the domain of terms that the indexer had to select from and thus which terms best describe the information needed.  Thus controlled vocabularies significantly reduce the miss match between the vocabulary of the author and the vocabulary of the searcher.  Uncontrolled vocabularies have the opposite effect, making indexing faster but the search process much more difficult. 	The availability of items in electronic form changes the objectives of manual indexing.  The source information (frequently called citation data) can au-tomatically be extracted.  The use of entity identification and availability of elec-tronic thesauri and other reference databases can compensate for diversity of lan-guage/vocabulary use and thus eliminate the need for controlled vocabularies.  . The primary use of manual subject indexing now shifts to abstraction of concepts and judgments on the value of the information.  The automatic text analysis algo-rithms cannot consistently perform abstraction on all concepts that are in an item. They cannot correlate the facts in an item in a cause/effect relationship to deter-mine additional related concepts to be indexed.  An item that is discussing the in-crease in water temperatures at factory discharge locations could also be providing information on “economic stability” of a country that has fishing as its major in-dustry.  It requires the associative capabilities of a human being to make the con-nection.  A computer system would typically not be able to correlate the changes in temperature to economic stability (e.g., use of categorization tool with econom-ic stability as a category). The additional index terms added if manual indexing (tagging) is allowed will enhance the recall capability of the system.  For certain queries it may also increase the precision. 	The words used in an item do not always reflect the value of the concepts being presented.  It is the combination of the words and their semantic implica-tions that contain the intelligence value of the concepts being discussed.  The utili-ty of a concept is also determined by the user’s need.  The Public File indexer needs to consider the information needs of all users of the library system.  Indi-vidual users of the system have their own domains of interest that bound the con-cepts in which they are interested.  It takes a human being to evaluate the quality of the concepts being discussed in an item to determine if that concept should be indexed.  The difference in “user need” between the library class of indexers and the individual users is why Private Index files are an essential part of any good in-formation system.   It allows the user to logically subset the total document file in-to folders of interest including only those documents that, in the user’s judgment, have future value.  It also allows the user to judge the utility of the concepts based upon his need versus the system need and perform concept abstraction.  Selective indexing based upon the value of concepts increases the precision of searches. 	Availability of full document indexing saves the indexer from entering index terms that are identical to words in the document.  In most private corporate systems users are given the opportunity to add additional index terms or their opinions on the information which is available to all other users. Sometimes the index terms are entered by professional indexers and in most other cases they are added by other users of the system.  This distinction is important because the sys-tem needs to protect the index data added by the professionals while allowing changes to that added by the users.   There is overlap between the Private and Pub-lic index terms added.  One of the changes that has come with the Internet is where some web sites are allowing any user to enter indexing.  In this case there are users responsible for the integrity of subsets of the site that review what other users enter and make adjustments as needed (Wikipedia).  This concept of collab-orative indexing has significantly expanded the knowledge base available for search systems and users.4.2	Manual Indexing Process	When an organization with multiple indexers decides to create a public or private index some procedural decisions on how to create the index terms assist the indexers and end users in knowing what to expect in the index file.  The first decision is the scope of the indexing to define what level of detail the subject in-dex will contain.  This is based upon usage scenarios of the end users.  The other decision is the need to link index terms together in a single index for a particular concept. 4.2.1	Scope of Indexing	When manual indexing is allowed to augment the automated indexing, the process of reliably and consistently determining the bibliographic terms that represent the concepts in an item is extremely difficult.  Problems arise from inter-action of two sources:  the author and the indexer.  The vocabulary domain of the author may be different than that of the indexer, causing the indexer to misinter-pret the emphasis and possibly even the concepts being presented.  The indexer is not an expert on all areas and has different levels of knowledge in the different ar-eas being presented in the item.  This results in different quality levels of indexing.  The indexer must determine when to stop the indexing process.	There are two factors involved in deciding on what level to index the concepts in an item: the exhaustivity and the specificity of indexing desired.  Ex-haustivity of indexing is the extent to which the different concepts in the item are indexed.  For example, if two sentences of a 10-page item on microprocessors discuss on-board caches, should this concept be indexed?  Specificity relates to the preciseness of the index terms used in indexing.  For example, whether the term “processor” or the term “microcomputer” or the term “Pentium” should be used in the index of an item is based upon the specificity decision.  Indexing an item only on the most important concept in it and using general index terms yields low ex-haustivity and specificity.  This approach requires a minimal number of index terms per item and reduces the cost of generating the index. For example, indexing this paragraph would only use the index term “indexing.”   High exhaustivity and specificity indexes almost every concept in the item using as many detailed terms as needed.  Under these parameters this paragraph would have “indexing,” “indexer knowledge,” “exhaustivity” and “specificity” as index terms. Low exhaustivity has an adverse effect on both precision and recall.  If the full text of the item is indexed, then low exhaustivity is used to index the abstract concepts not explicit in the item with the expectation that the typical query searches both the index and the full item index.  Low specificity has an adverse effect on precision, but no effect to a potential increase in recall.Another decision on indexing is what portions of an item should be in-dexed.  The simplest case is to limit the indexing to the Title or Title and Abstract zones.  This indexes the material that the author considers most important and re-duces the costs associated indexing an item.  This leads to loss of recall.	Weighting of index terms is not common in manual indexing systems. Weighting is the process of assigning an importance to an index term’s use in an item.  The weight should represent the degree to which the concept associated with the index term is represented in the item. The weight should help in discrimi-nating the extent to which the concept is discussed in items in the database.  4.2.2	Precoordination and Linkages	Another decision on the indexing process is whether linkages are availa-ble between index terms for an item.  Linkages are used to correlate related at-tributes associated with concepts discussed in an item. This process of creating term linkages at index creation time is called precoordination.   When index terms are not coordinated at index time, the coordination occurs at search time.  This is called post coordination that is coordinating terms after (post) the indexing process.  Post coordination is implemented by “AND”ing index terms together, which only find indexes that have all of the search terms.  		INDEX TERMS	Methodology	oil, wells, Mexico, CITGO, refineries, Peru, BP, drilling	            No linking of terms	(oil wells, Mexico, drilling, CITGO)	(U.S.,oil refineries, Peru, introduction)	            linked (Precoordination) 	(CITGO, drill, oil wells, Mexico)(U.S., introduction, oil refineries, Peru)	            linked (Precoordination)            with position indicating role	(SUBJECT: CITGO;	                    ACTION: drilling;	   OBJECT: oil,wellsMODIFIER: in Mexico) (SUBJECT:U.S.; ACTION: introduces;OBJECT: oil refineries;MODIFIER: in Peru)	            linked (Pre-coordination)            with modifier indicating role				      Figure 4.1  Linkage of Index TermsFactors that must be determined in the linkage process are the number of terms that can be related, any ordering constraints on the linked terms, and any additional descriptors are associated with the index terms (Vickery-70).  The range of the number of index terms that can be linked is not a significant implementation issue and primarily affects the design of the indexer’s user interface.  When multi-ple terms are being used, the possibility exists to have relationships between the terms.  For example, the capability to link the source of a problem, the problem and who is affected by the problem may be desired.  Each term must be caveated with one of these three categories along with linking the terms together into an in-stance of the relationships describing one semantic concept.  The order of the terms is one technique for providing additional role descriptor information on the index terms.  Use of the order of the index terms to implicitly define additional term descriptor information limits the number of index terms that can have a role descriptor.  If order is not used, modifiers may be associated with each term linked to define its role.  This technique allows any number of terms to have the associat-ed role descriptor.   Figure 4.1 shows the different types of linkages. It assumes that an item discusses the drilling of oil wells in Mexico by CITGO and the intro-duction of oil refineries in Peru by the U.S.  When the linked capability is added, the system does not erroneously relate Peru and Mexico since they are not in the same set of linked items.  It still does not have the ability to discriminate between which country is introducing oil refineries into the other country.  Introducing roles in the last two examples of Figure 4.1 removes this ambiguity.  Positional roles   treat the data as a vector allowing only one value per position.  Thus if the example is expanded so that the U.S. was introducing oil refineries in Peru, Boliv-ia and Argentina, then the positional role technique would require three entries, where the only difference would be in the value in the “affected country” position.  When modifiers are used, only one entry would be required and all three countries would be listed with three “MODIFIER”s. 4.3  Automatic Indexing of Text	Automatic indexing is the capability for the system to automatically de-termine the index terms to be assigned to an item.   The simplest case is when all words in the document are used as possible index terms (total document indexing).   More complex processing is required when the objective is to emulate a human indexer and determine a limited number of index terms for the major concepts in the item.  As discussed, the advantages of human indexing are the ability to determine concept abstraction and judge the value of a concept.  The disadvantages of human indexing over automatic indexing are cost, processing time and consistency.  Once the initial hardware cost is amortized, the costs of automatic indexing are absorbed as part of the normal operations and maintenance costs of the computer system.  There are no additional indexing costs versus the salaries and benefits regularly paid to human indexers.	Processing time of an item by a human indexer varies significantly based upon the indexer’s knowledge of the concepts being indexed, the exhaustivity and specificity guidelines and the amount and accuracy of preprocessing via Automat-ic File Build. Even for relatively short items (e.g., 300 - 500 words) it normally takes at least five minutes per item.  A significant portion of this time is caused by the human interaction with the computer (e.g., typing speeds, cursor positioning, correcting spelling errors, and taking breaks between activities).  Automatic index-ing requires only a few seconds or less of computer time based upon the size of the processor and the complexity of the algorithms to generate the index. 	Another advantage to automatic indexing is the predictably of algorithms.  If the indexing is being performed automatically, by an algorithm, there is con-sistency in the index term selection process.  Human indexers typically generate different indexing for the same document.  In an experiment on consistency in TREC-2, there was, on the average, a 20 per cent difference in judgment of the same item’s topics between the original and a second independent judge of over 400 items (Harman-95).  Since the judgments on relevance are different, the selec-tion of index terms and their weighting to reflect the topics is also different.  In au-tomatic indexing, a sophisticated researcher understands the automatic process and be able to predict its utility and deficiencies, allowing for compensation for system characteristics in a search strategy.  Even the end user, after interacting with the system, understands for certain classes of information and certain sources, the ability of the system to find relevant items is worse than other classes and sources.  For example, the user may determine that searching for economic issues is far less precise than political issues in a particular newspaper based information system.  The user may also determine that it is easier to find economic data in an infor-mation database containing Business Weekly than the newspaper source.	Automatic indexing is the process of analyzing an item to extract the in-formation to be permanently kept in an index.  This process is associated with the generation of the searchable data structures associated with an item.  The first step in automatic indexing is associated with the Ingest process described in Chapter 3 where the structure of the item (e.g., zoning) and the processing tokens to be used in the indexing process are determined.  In addition to the structure that is associ-ated the document, in order to improve precision some systems automatically di-vide the document up into fixed length passages or localities, which become the item unit that is indexed (Kretser-99.)   Having shorter logical documents limits the number of words that are associated with that item and thus avoids false hits that come from words separated by distances (e.g., paragraphs apart) that are unre-lated still causing the retrieval of an item.  The shorter items are only for search purposes in all cases the original item is retrieved.  Figure 4.2 shows the indexing process and the associated search and retrieval process.  After the searchable index is created the user will issue a search statement.  The Hit List generated from the search will have pointers to the original documents so they can be displayed upon request from the user.  Figure 4.2 Index and search processing Flow	Indexes resulting from automated indexing fall into two classes: weighted and unweighted.  In an unweighted indexing system, the existence of an index term in a document and sometimes its word location(s) are kept as part of the searchable data structure.  No attempt is made to discriminate between the values of the index terms in representing concepts in the item (i.e., they are all the same weight).  Looking at the index, it is not possible to tell the difference between the main topics in the item and a casual reference to a concept.  This architecture is typical of the commercial systems through the 1980s.  Queries against unweighted systems are based upon Boolean logic and the items in the resultant Hit file are considered equal in value.  The last item presented in the Hit file is as likely as the first item to be relevant to the user’s information need.	To understand how automatic weighting occurs the first thing to consider is what information is available to the system to do the automatic indexing.  There are three things available; the current item and the processing tokens in it, there is the database so far built and there could be other databases of reference infor-mation (e.g., in-link/out-links of web sites to judge a sites importance).  In a weighted indexing system, an attempt is made to place a value on the index term’s representation of its associated concept in the document.  The most direct evi-dence to be used in weighting an item is the item itself and the frequency of occur-rence of words in the item.  Luhn, one of the pioneers in automatic indexing, in-troduced the concept of the “resolving power” of a term. Luhn postulated that the significance of a concept in an item is directly proportional to the frequency of use of the word associated with the concept in the document (Luhn-58, Salton-75). This is reinforced by the studies of Brookstein, Klein and Raita that show “content bearing” words are not randomly distributed (i.e., Poisson distributed), but that their occurrence “clump” within items (Brookstein-95). Typically, values for the index terms are normalized between zero and one.  The higher the weight, the more the term represents a concept discussed in the item.  The weight can be ad-justed to account for other information such as the number of items in the database that contain the same concept.  Although this process was initially applied to each processing token as if each was independent of its neighbors, some systems also consider word phases (i.e., Contiguous Word Phrases) as a searchable unit and ap-ply similar techniques to weighting them.  This initial weight is then adjusted by information across the database (such as how often the word occurs in other items and/or the other external databases of support information.The query process uses the weights along with any weights assigned to terms in the query to determine a scalar value (rank value) used in predicting the likelihood that an item satisfies the query.  Thresholds or a parameter specifying the maximum number of items to be returned is used to bind the number of items returned to a user. The results are presented to the user in order of the rank value from highest number to lowest number. There are three major approaches to generation of the searchable index; statistical, natural language, and concept.  	Statistical strategies cover the broadest range of indexing techniques and are the most prevalent in commercial systems.   The basis for a statistical approach is use of frequency of occurrence of events.  The events usually are related to occurrences of processing tokens (words/phrases) within documents and within the database.  The words/phrases are the domain of searchable values.  The statistics that are applied to the event data are probabilistic, Bayesian, and vector space. The static approach stores a single statistic, such as how often each word occurs in an item that is used in generating relevance scores after a standard Boolean search. Probabilistic indexing stores the information that are used in calculating a probability that a particular item satisfies (i.e., is relevant to) a particular query.  Bayesian and vector approaches store information used in generating a relative confidence level of an item’s relevance to a query.  In addition the Bayesian approach is probabilistic, but to date the developers of this approach are more focused on a good relative relevance value than producing and absolute probability. Neural networks are dynamic learning structures that are also discussed under concept indexing where they are used to determine concept classes.	Natural Language approaches perform similar processing token identification as in statistical techniques, but then additionally perform varying levels of natural language parsing of the item.  This parsing disambiguates the context of the processing tokens and generalizes to more abstract concepts within an item (e.g., present, past, future actions).  This additional information is stored within the index to be used to enhance the search precision.	Concept indexing uses the words within an item to correlate to concepts discussed in the item.  This is a mapping of the specific words to a new set of “concept words” used to index the item.  When generating the concept words au-tomatically, there is not a name applicable to the concept but just a statistical significance. 	Finally, there is an extension of what is being indexed associated with the existence of hypertext linkages.  These linkages provide virtual threads of con-cepts between items versus directly defining the concept within an item.4.3.1 Statistical Indexing	Statistical indexing uses frequency of occurrence of events to calculate a number that is used to indicate the potential relevance of an item. One approach used in search of older systems does not use the statistics to aid in the initial selec-tion, but uses them to assist in calculating a relevance value of each item for rank-ing.  The documents are found by a normal Boolean search and then statistical calculations are performed on the Hit file, ranking the output (e.g., term frequency algorithms).  Probabilistic systems attempt to calculate a probability value that should be invariant to both calculation method and text corpora.  This allows easy inte-gration of the final results when searches are performed across multiple databases and use different search algorithms.  A probability of 50 per cent would mean that if enough items are reviewed, on the average one half of the reviewed items are relevant.  The Bayesian and Vector approaches calculate a relative relevance value (i.e., confidence level) that a particular item is relevant.  Quite often term distribu-tions across the searchable database are used in the calculations.  An issue that continues to be researched is how to merge results, even from the same search al-gorithm, from multiple databases.  The problem is compounded when an attempt is made to merge the results from different search algorithms. This would not be a problem if true probabilities versus confidence levels were calculated. 4.3.1.1 Probabilistic Weighting	The probabilistic approach is based upon direct application of the theory of probability to information retrieval systems.  This has the advantage of being able to use the developed formal theory of probability to direct the algorithmic de-velopment.  It also leads to an invariant result that facilitates integration of results from different databases.  The use of probability theory is a natural choice because it is the basis of evidential reasoning (i.e., drawing conclusions from evidence).  This is summarized by the Probability Ranking Principle (PRP) and its Plausible Corollary (Cooper-94):HYPOTHESIS:  If a reference retrieval system’s response to each request is a ranking of the documents in the collection in order of decreasing probability of usefulness to the user who submitted the request, where the probabilities are esti-mated as accurately as possible on the basis of whatever data is available for this purpose, then the overall effectiveness of the system to its users is the best obtainable on the basis of that data.PLAUSIBLE COROLLARY:  The most promising source of techniques for estimating the probabilities of usefulness for output ranking in IR is standard probability theory and statis-tics.There are several factors that make this hypothesis and its corollary difficult (Gor-don-92, Gordon-91, Robertson-77).  Probabilities are usually based upon a binary condition; an item is relevant or not.  But in information systems the relevance of an item is a continuous function from non-relevant to absolutely useful.  A more complex theory of expected utility (Cooper-78) is needed to address this charac-teristic.  Additionally, the output ordering by rank of items based upon probabili-ties, even if accurately calculated, may not be as optimal as that defined by some domain specific heuristic (Stirling-77).  The domains in which probabilistic rank-ing are suboptimal are so narrowly focused as to make this a minor issue.  But these issues mentioned are not as compelling as the benefit of a good probability value for ranking that would allow integration of results from multiple sources.  There is an assumption in probability theory that terms are independent of each other, but proximity of terms in textual items affects their potential aggregate weight in representing a concept which is the ultimate goal.	The source of the problems that arise in application of probability theory come from a lack of accurate data and simplifying assumptions that are applied to the mathematical model.  If nothing else, these simplifying assumptions cause the results of probabilistic approaches in ranking items to be less accurate than other approaches.  The advantage of the probabilistic approach is that it can accurately identify its weak assumptions and work to strengthen them.  In many other ap-proaches, the underlying weaknesses in assumptions are less obvious and harder to identify and correct.  Even with the simplifying assumption, results from compari-sons of approaches in the TREC conferences have shown that the probabilistic ap-proaches, while not scoring highest, are competitive against all other approaches.	There are many different areas in which the probabilistic approach may be applied.  The method of logistic regression is described as an example of how a probabilistic approach is applied to information retrieval (Gey-94).  The approach starts by defining a “Model 0” system which exists before specific probabilistic models are applied.  In a retrieval system there exist query terms qi and document terms di, which have a set of attributes (v1, . . . , vn) from the query (e.g., counts of term frequency in the query), from the document (e.g., counts of term frequency in the document) and from the database (e.g., total number of documents in the data-base divided by the number of documents indexed by the term).	The logistic reference model uses a random sample of query-document-term triples for which binary relevance judgments have been made from a training sample.  Log O is the logarithm of the odds (logodds) of relevance for term tk which is present in document Dj and query Qi:log(O(R | Qi, Dj, tk)) = c0 + c1v1 + . . . + cnvnThe logarithm that the ith Query is relevant to the jth Document is the sum of the logodds for all terms:log(O(R | Qi, Dj)) =  [log(O(R | Qi, Dj, tk)) - log(O(R))]where O(R) is the odds that a document chosen at random from the database is relevant to query Qi.  The coefficients c are derived using logistic regression which fits an equation to predict a dichotomous independent variable as a function of independent variables that show statistical variation (Hosmer-89).  The inverse logistic transformation is applied to obtain the probability of relevance of a docu-ment to a query:P(R | Qi , Dj) =  1 \ (1 + e - log(O(R | Qi, Dj)) )	The coefficients of the equation for logodds is derived for a particular database us-ing a random sample of query-document-term-relevance quadruples and used to predict odds of relevance for other query-document pairs.	Gey applied this methodology to the Cranfield Collection (Gey-94). The collection has 1400 items and 225 queries with known results.  Additional attrib-utes of relative frequency in the query (QRF), relative frequency in the document (DRF) and relative frequency of the term in all the documents (RFAD) were in-cluded, producing the following logodds formula:Zj = log(O(R | tj)) = c0+ c1log(QAF) + c2log(QRF) + c3log(DAF) + c4log(DRF)		+  c5log(IDF) + c6log(RFAD)where QAF, DAF, and IDF were previously defined,  QRF = QAF\ (total number of terms in the query), DRF = DAF\(total number of words in the document) and RFAD = (total number of term occurrences in the database)\ (total number of all words in the database).  Logs are used to reduce the impact of frequency infor-mation; then smooth out skewed distributions.  A higher maximum likelihood is attained for logged attributes.	The coefficients and log (O(R)) were calculated creating the final formula for ranking for query vector  , which contains q terms:log(O(R | )) = -5.138 +  (Zj + 5.138)	The logistic inference method was applied to the test database along with the Cornell SMART vector system which uses traditional term frequency, inverse document frequency and cosine relevance weighting formulas (see Section 5.2.2).  The logistic inference method outperformed the vector method.	Thus the index that supports the calculations for the logistic reference model contains the O(R) constant value (e.g., -5.138) along with the coefficients c0 through c6.  Additionally, it needs to maintain the data to support DAF, DRF, IDF and RFAD.  The values for QAF and QRF are derived from the query.	Attempts have been made to combine the results of different probabilistic techniques to get a more accurate value.  The objective is to have the strong points of different techniques compensate for weaknesses.  To date this combination of probabilities using averages of Log-Odds has not produced better results and in many cases produced worse results (Hull-96).	The Okapi weighting model that started with TREC in the 1990s tries to adjust the standard probability models to account for the variances in textual doc-uments that were causing many of the errors in a probabilistic approach.  It is probably the best developed probabilistic approach to information retrieval.   4.3.1.2 Baysean Indexing Another Probabilistic model that has been most successful in this area is the Bayesian approach.  This approach is natural to information systems and is based upon the theories of evidential reasoning (drawing conclusions from evi-dence).  One way of overcoming the restrictions inherent in a vector model is to use a Bayesian approach to maintaining information on processing tokens.  The Bayesian model provides a conceptually simple yet complete model for information systems.  In its most general definition, the Bayesian approach is based upon conditional probabilities (e.g., Probability of Event 1 given Event 2 occurred).  This general concept can be applied to the search function as well as to creating the index to the database.  The objective of information systems is to return relevant items.  Thus the general case, using the Bayesian formula, is P (REL/DOCi, Queryj) which is interpreted as the probability of relevance (REL) to a search statement given a particular document and query.  In addition to search, Bayesian formulas can be used in determining the weights associated with a particular processing token in an item.  The objective of creating the index to an item is to represent the semantic information in the item.  A Bayesian network can be used to determine the final set of processing tokens (called topics) and their weights.  Figure 4.3 shows a simple view of the process where Ti represents the relevance of topic “i” in a particular item and Pj represents a statistic associated with the event of processing token “j” being present in the item. Figure 4.3  Bayesian Term WeightingThe “m” topics would be stored as the final index to the item.  The statistics asso-ciated with the processing token are typically frequency of occurrence.  But they can also incorporate proximity factors that are useful in items that discuss multiple topics.  There is one major assumption made in this model:Assumption of Binary Independence: the topics and the processing token statistics are independent of each other.  The existence of one topic is not related to the existence of the other topics.  The existence of one pro-cessing token is not related to the existence of other processing tokens.In most cases this assumption is not true. Some topics are related to other topics and some processing tokens related to other processing tokens.  For example, the topics of “Politics” and “Economics” are in some instances related to each other (e.g., an item discussing Congress debating laws associated with balance of trade) and in many other instances totally unrelated.  The same type of example would apply to processing tokens.  There are two approaches to handling this problem.  The first is to assume that there are dependencies, but that the errors introduced by assuming the mutual independence do not noticeably effect the determination of relevance of neither an item nor its relative rank associated with other retrieved items.  This is the most common approach used in system implementations.  A second approach can extend the network to additional layers to handle interde-pendencies.  Thus an additional layer of Independent Topics (ITs) can be placed above the Topic layer and a layer of Independent Processing Tokens (IPs) can be placed above the processing token layer. Figure 4.4 shows the extended Bayesian network. Extending the network creates new processing tokens for those cases where there are dependencies between processing tokens.  The new set of Inde-pendent Processing Tokens can then be used to define the attributes associated with the set of topics selected to represent the semantics of an item.   Figure 4.4  Extended Bayesian NetworkTo compensate for dependencies between topics the final layer of Independent Topics is created.  The degree to which each layer is created depends upon the er-ror that could be introduced by allowing for dependencies between Topics or Pro-cessing Tokens.  Although this approach is the most mathematically correct, it suf-fers from losing a level of precision by reducing the number of concepts available to define the semantics of an item.4.3.1.3 Vector Weighting	One of the earliest systems that investigated statistical approaches to in-formation retrieval was the SMART system at Cornell University (Buckley-95, Salton-83).  The system is based upon a vector model.  The semantics of every item are represented as a vector.  A vector is a one-dimensional set of values, where the order/position of each value in the set is fixed and represents a particular domain.  In information retrieval, each position in the vector typically represents a processing token.  There are two approaches to the domain of values in the vector: binary and weighted.  Under the binary approach, the domain contains the value of one or zero, with one representing the existence of the processing token in the item.  In the weighted approach, the domain is typically the set of all real positive numbers.  The value for each processing token represents the relative importance of that processing token in representing the se-mantics of the item.  Figure 4.5 shows how an item that discusses petroleum refineries in Mexico would be represented.  In the example, the major topics discussed are indicated by the index terms for each column (i.e., Petroleum, Mexico, Oil, Taxes, Refineries and Shipping).	Binary vectors require a decision process to determine if the degree that a particular processing token represents the semantics of an item is sufficient to in-clude it in the vector.  In the example for Figure 4.5, a five-page item may have had only one sentence like “Standard taxation of the shipment of the oil to refiner-ies is enforced.”  For the binary vector, the concepts of “Tax” and  “Shipment” are below the threshold of importance (e.g., assume threshold is 1.0) and they not are included in the vector.  	       Petroleum    Mexico   Oil    Taxes    Refineries    Shipping  Binary		 1      ,        1      ,   1  ,       0   ,       1        ,         0      Weighted           2.8    ,        1.6   ,    3.5 ,     .3   ,       3.1    ,         .1    Figure 4.5   Binary and Vector Representation of an ItemA weighted vector acts the same as a binary vector but it provides a range of val-ues that accommodates a variance in the value of the relative importance of a pro-cessing token in representing the semantics of the item.  The use of weights also provides a basis for determining the rank of an item.  	The vector approach allows for a mathematical and a physical representa-tion using a vector space model.  Each processing token can be considered another dimension in an item representation space.  In Chapter 5 it is shown that a query can be represented as one more vector in the same n-dimensional space.  Figure 4.6 shows a three-dimensional vector representation assuming there were only three processing tokens, Petroleum, Mexico and Oil.              Mexico                   1.6							           3.5                  		                                                                                  Oil2.8PetroleumFigure 4.6  Vector Representation	The original document vector is typically extended by additional infor-mation such as citations/references and some times categorization and entity ex-tracted values to add more information for search and clustering purposes.  The ci-tation information is not weighted because it’s more like database facts (e.g., date of publication).There are many algorithms that can be used in calculating the weights used to represent a processing token.  Part of the art in information retrieval is de-riving changes to the basic algorithms to account for normalization (e.g., account-ing for variances in number of words in items). The following subsections present the major algorithms starting with the most simple term frequency algorithm.4.3.1.3.1  Simple Term Frequency Algorithm 	An automatic indexing process implements an algorithm to determine the weight to be assigned to a processing token for a particular item.  Looking at a typical textual item, the data elements that are potentially available for calculating a weight are the frequency of occurrence of the processing token in an existing item (i.e., term frequency - TF), the frequency of occurrence of the processing to-ken in the existing database (i.e., total frequency - TOTF) and the number of unique items in the database that contain the processing token (i.e., item frequency - IF, frequently labeled in other publications as document frequency - DF).   As discussed previously, the premises by Luhn and later Brookstein that the resolving power of content-bearing words is directly proportional to the frequency of occur-rence of the word in the item is used as the basis for most automatic weighting techniques.  Weighting techniques usually are based upon positive weight values.	All algorithms start with the approach to have the weight equal to the term frequency. This approach emphasizes the use of a particular processing token within an item.  Thus if the word “computer” occurs 15 times within an item it has a weight of 15.  The simplicity of this technique encounters problems of normali-zation between items and use of the processing token within the database.  The longer an item is, the more often a processing token may occur within the item.  Use of the absolute value biases weights toward longer items, where a term is more likely to occur with a higher frequency.  One normalization typically used in weighting algorithms compensates for the number of words in an item.  	An example of this normalization in calculating term-frequency is the al-gorithm used in the SMART System at Cornell (Buckley-96). The term frequency weighting formula used in TREC 4 was:		(1 + log(TF))/1 + log(average (TF))                          (1 - slope) ∗ pivot + slope ∗ number of unique termswhere slope was set at .2 and the pivot was set to the average number of unique terms occurring in the collection (Singhal-95).  In addition to compensating for document length, they also want the formula to be insensitive to anomalies intro-duced by stemming or misspellings.  	Although initially conceived of as too simple, experiments by the SMART system using the large databases in TREC demonstrated that use of the simpler algorithm with proper normalization factors is far more efficient in pro-cessing queries and return hits similar to more complex algorithms. 	There are many approaches to account for different document lengths when normalizing the value of Term Frequency to use (e.g., an items that is only 50 words may have a much smaller term frequency then and item that is 1000 words on the same topic).  In the first technique, the term frequency for each word is divided by the maximum frequency of the word in any item.  This normalizes the term frequency values to a value between zero and one.  This technique is called “maximum term frequency”.  The problem with this technique is that the maximum term frequency in just one item in the database can be so large that it decreases the value of term frequency in short items to too small a value and loses significance.  Additionally as new items are added to the database there is a ques-tion what to do with all of the currently processed items if one of them has a new larger maximum term frequency.  A simpler version of this takes the maximum term frequency of any word within an item and divides all the term frequencies in the item by it – thus getting relative values between zero and one.  Once again this can be distorted if on word has a very large term frequency within an item.	Another option is to use logaritmetic term frequency.  In this technique the log of the term frequency plus a constant is used to replace the term frequency.  The log function will perform the normalization when the term frequencies vary significantly due to size of documents.  Along this line the COSINE function used as a similarity measure (see Chapter 5) can be used to normalize values in a doc-ument.  This is accomplished by treating the index of a document as a vector and divide the weights of all terms by the length of the vector.  This will normalize to a vector of maximum length one.  This uses all of the data in a particular item to perform the normalization and will not be distorted by any particular term. The problem occurs when there are multiple topics within an item. The COSINE tech-nique will normalize all values based upon the total length of the vector that repre-sents all of topics. If a particular topic is important but briefly discussed, its nor-malized value could significantly reduce its overall importance in comparison to another document that only discusses the topic. 	Another approach recognizes that the normalization process may be over penalizing long documents (Singhal-95).  Singhal did experiments that showed longer documents in general are more likely to be relevant to topics then short documents.  Yet normalization was making all documents appear to be the same length.  To compensate, a correction factor was defined that is based upon docu-ment length that maps the Cosine function into an adjusted normalization function.  The function determines the document length crossover point for longer docu-ments where the probability of relevance equals the probability of retrieval. (given a query set).  This value called the "pivot point" is used to apply an adjustment to the normalization process.  The theory is based upon straight lines so it is a matter of determining slope of the lines.New normalization = (slope)*(old normalization) + KK is generated by the rotation of the pivot point to generate the new line and the old normalization = the new normalization at that point.  The slope for all higher values will be different.  Substituting pivot for both old and new value in the above formula we can solve for K at that point.  Then using the resulting formula for K and substituting in the above formula produces the following formula:Pivoted function = slope)*(old normalization) + (1.0 - slope)*(pivot)Slope and pivot are constants for any document/query set.   Another problem is that the Cosine function favors short documents over long documents and also fa-vors documents with a large number of terms.  This favoring is increased by using the pivot technique.  If log(TF) is used instead of the normal frequency then TF is not a significant factor.  In documents with large number of terms the Cosine fac-tor is approximated by the square root of the number of terms.  This suggests that using the ratio of the logs of term frequencies would work best for longer items in the calculations:(1 + log(TF))/(1 + log(average(TF))This leads to the final algorithm that weights each term by the above formula di-vided by the pivoted normalization:((1 + log(TF))/(1 + log(average(TF))/(slope)(No. unique terms) + (1-slope)*(pivot)Singhal demonstrated the above formula works better against TREC data then TF/MAX(TF) or vector length normalization.  The effect of a document with a high term frequency is reduced by the normalization function by dividing the TF by the average TF and by use of the log function.  The use of pivot normalization adjusts for the bias towards shorter documents increasing the weights of longer documents.4.3.1.3.2 Inverse Document Frequency	Once the frequency within an item has been normalized the weighting can be improved by taking into consideration the frequency of occurrence of the processing token in the database.  One of the objectives of indexing an item is to discriminate the semantics of that item from other items in the database.  If the to-ken “computer” occurs in every item in the database, its value representing the semantics of an item may be less useful compared to a processing token that oc-curs in only a subset of the items in the database. The term “computer” represents a concept used in an item, but it does not help a user find the specific information being sought since it returns the complete database. This leads to the general statement enhancing weighting algorithms that the weight assigned to an item should be inversely proportional to the frequency of occurrence of an item in the database. This is referred to as Shannon’s information theory law. The algorithm based upon this idea is called inverse document frequency (IDF).  The un-normalized weighting formula is:WEIGHTij =  TFij ∗  [Log2(n) - Log2(IFj ) + 1]	 where WEIGHTij is the vector weight that is assigned to term “j” in item “i,” TFij (term frequency) is the frequency of term “j” in item “i” , “n” is the number of items in the database and IFj (item frequency or document frequency) is the num-ber of items in the database that have term “j” in them.  A negative log is the same as dividing by the log value,  thus the basis for the name of the algorithm.  Figure 5.4 demonstrates the impact of using this weighting algorithm. The term “refin-ery” has the highest frequency in the new item (10 occurrences). But it has a nor-malized weight of 20 which is less than the normalized weight of “Mexico.”   This change in relative importance between “Mexico” and “refinery” from the unnor-malized to normalized vectors is due to an adjustment caused by “refinery”  al-ready existing in 50 per cent of the database versus “Mexico” which is found in 6.25 per cent of the items.	The major factor of the formula for a particular term is (Log2(n) - Log2(IFj)).  The value for IF can vary from “1” to “n.”  At “n,” the term is found in every item in the database and the factor becomes (Log2(n) - Log2(n)) = 1.  As the number of items a term is found in decreases, the value of the denominator de-creases eventually approaching the value Log2(1) which is close to 1. The weight assigned to the term in the item varies from   Tfi,j ∗ (1 + 1) to Tfi,j ∗ (∼ Log2(n)).  The effect of this factor can be too great as the number of items that a term is found in becomes small.  To compensate for this, the INQUERY system at the University of Massachusetts normalizes this factor by taking an additional log value.	The value of “n” and IFi vary as items are added and deleted from the da-tabase. To implement this algorithm in a dynamically changing system, the physi-cal index only stores the frequency of occurrence of the terms in an item (usually with their word location) and the IDF factor is calculated dynamically at retrieval time.  The required information can easily be determined from an inversion list for a search term that is retrieved and a global variable on the number of items in the database.   The following example shows how inverse document frequency works.Assume that the term “oil” is found in 128 items,  “Mexico” is found in 16 items and “refinery” is found in 1024 items.  If a new item arrives with all three terms in it, “oil” found 4 times, “Mexico” found 8 times, and “refinery  found 10 times and there are 2048 items in the total data-base, Figure 5.4 shows the weight calculations using inverse document frequency. Using a simple unnormalized term frequency, the item vector is (4, 8, 10)Using inverse document frequency the following calculations apply:Weightoil = 4 ∗ (Log2(2048) - Log2(128) + 1) = 4 ∗ ( 11 - 7 + 1) = 20WeightMexico = 8 ∗ (Log2(2048) - Log2(16) + 1) = 8 ∗ (11 - 4 + 1) =  64 Weightrefinery = 10 ∗ (Log2(2048) - Log2(1024) + 1) = 			          10 ∗ (11 - 10 + 1 ) = 20with the resultant inverse document frequency item vector = (20, 64, 20)4.3.1.3.3  Signal Weighting	Inverse document frequency adjusts the weight of a processing token for an item based upon the number of items that contain the term in the existing data-base.  What it does not account for is the term frequency distribution of the pro-cessing token in the items that contain the term.  The distribution of the frequency of processing tokens within an item can affect the ability to rank items.  For ex-ample, assume the terms “SAW” and “DRILL” are found in 5 items with the fol-lowing frequencies defined in Figure 4.7.Both terms are found a total of 50 times in the five items.  The term “SAW” does not give any insight into which item is more likely to be relevant to a search of “SAW”.  If precision is a goal (maximizing relevant items shown first), then the weighting algorithm could take into consideration the non-uniform distri-bution of term “DRILL” in the items that the term is found,  applying even higher weights to it than “SAW.”  The theoretical basis for the algorithm to emphasize precision is Shannon’s work on Information Theory (Shannon-51).Item Distribution			SAW		DRILL  A				10		    2  B				10		    2  C				10		    18  D				10		    10  E				10		    18Figure 4.7  Item Distribution for SAW and DRILL	In Information Theory, the information content value of an object is in-versely proportional to the probability of occurrence of the item.  An instance of an event that occurs all the time has less information value than an instance of a seldom occurring event.  This is typically represented as INFORMATION = -Log2(p), where p is the probability of occurrence of event  “p.”   The information value for an event that occurs .5 per cent of the time is: INFORMATION = - Log2(.005)		     = - (-7.64)		     = 7.64   The information value for an event that occurs 50 per cent of the time is: INFORMATION = - Log2 (.50)		      = -(-1)		      =  1If there are many independent occurring events then the calculation for the aver-age information value across the events is:AVE_INFO = -   pk Log2 (pk)The value of AVE_INFO takes its maximum value when the values for every pk is the same.  Its value decreases proportionally to increases in variances in the values of pk.  The value of pk can be defined as TFik/TOTFk, the ratio of the frequency of occurrence of the term in an item to the total number of occurrences of the item in the data base.  Using the AVE_INFO formula, the terms that have the most uni-form distribution in the items that contain the term have the maximum value.  To use this information in calculating a weight, the formula needs the inverse of AVE_INFO, where the minimum value is associated with uniform distributions and the maximum value is for terms that have large variances in distribution in the items containing the term. The following formula for calculating the weighting factor called Signal (Dennis-67) can be used:	Signalk = Log2 (TOTF) - AVE_INFO     producing a final formula of:Weightik = TFik  ∗ SignalkWeightik= TFik ∗ [Log2(TOTFk)  -  - TFik/TOTFk Log2 (TFik/TOTFk)]An example of use of the weighting factor formula is given for the values in Fig-ure 5.5:SignalSAW = LOG2 (50) - [5 ∗ {10/50LOG2(10/50)} ]                 = 5.64 – (5*(.2* (2.32) = 5.64 – 2.32 = 2.68SignalDRILL = LOG2 (50) - [2/50LOG2(2/50) + 2/50LOG2(2/50) + 	18/50LOG2(18/50) + 10/50LOG2(10/50) + 18/50LOG2(18/50)                   = 5.64 – (.04 * (4.64) + .04* (4.64)  + .36 (1.47) +  (.2* (2.32) + (.36 (1.47)) = 5.64 – (.186 + .186 + .053 +.464 + .053)                  = 5.64 - .942 = 4.058The weighting factor for term “DRILL” that does not have a uniform distribution is larger than that for term “SAW” and gives it a higher weight.	This technique could be used by itself or in combination with inverse document frequency or other algorithms.  The overhead of the additional data needed in an index and the calculations required to get the values have not been demonstrated to produce better results than other techniques and are not used in any systems at this time.  It is a good example of use of Information Theory in de-veloping information retrieval algorithms.  Effectiveness of use of this formula can be found in results from Harman and also from Lockbaum and Streeter (Har-man-86, Lochbaum-89).4.3.1.3.4  Discrimination Value	Another approach to creating a weighting algorithm is to base it upon the discrimination value of a term.  To achieve the objective of finding relevant items, it is important that the index discriminates among items.  The more all items ap-pear the same, the harder it is to identify those that are needed.  To use this ap-proach a “similarity” function is needed that can compare two Items and deter-mine how similar they are.   When the function is applied to the two Items the results should be a value between zero and one, where zero would mean there is no similarity between the Items and 1 suggests that the items are the same.  Exam-ples of these functions are in Chapter 5. Salton and Yang (Salton-73) proposed a weighting algorithm that takes into consideration the ability for a search term to discriminate among items. They proposed use of a discrimination value for each term “i”:DISCRIMi = AVESIMi  -  AVESIM where AVESIM is the average similarity between every item in the database and AVESIMi is the same calculation except that term “i” is removed from all items. AVESIM is calculated by summing the similarity between every Item and every other Item in the database and then divide by a normalization number.  There are three possibilities with the DISCRIMi value being positive, close to zero or nega-tive.  A positive value indicates that removal of term “i” has increased the simi-larity between items.  In this case, leaving the term in the database assists in dis-criminating between items and is of value.  A value close to zero implies that the term’s removal or inclusion does not change the similarity between items.  If the value of DISCRIMi is negative, the term’s effect on the database is to make the items appear more similar since their average similarity decreased with its remov-al.  Once the value of DISCRMi is normalized as a positive number, it can be used in the standard weighting formula as:Weightik = TFik  ∗ DISCRIMk4.3.1.3.5  Problems With Weighting Schemes	Often weighting schemes use information that is based upon processing token distributions across the database.  The two weighting schemes, inverse document frequency and signal,  use total frequency and item frequency factors which makes them dependent upon distributions of processing tokens within the database.  Information databases tend to be dynamic with new items always being added and to a lesser degree old items being changed or deleted.  Thus these fac-tors are changing dynamically.  There are a number of approaches to compensate for the constant changing values.a.  Ignore the variances and calculate weights based upon current values, with the factors changing over time.  Periodically rebuild the complete search database.b.  Use a fixed value while monitoring changes in the factors.  When the changes reach a certain threshold, start using the new value and update all existing vectors with the new value.c.  Store the invariant variables (e.g., term frequency within an item) and at search time calculate the latest weights for processing tokens in items needed for search terms. In the first approach the assumption minimizes the system overhead of maintain-ing currency on changing values, with the effect that term weights for the same term vary from item to item as the aggregate variables used in calculating the weights based upon changes in the database  vary over time.  Periodically the da-tabase and all term weights are recalculated based upon the most recent updates to the database.  For large databases in the millions of items, the overhead of rebuild-ing the database can be significant.  In the second approach, there is recognition that for the most frequently occurring items, the aggregate values are large.  As such, minor changes in the values have negligible effect on the final weight calcu-lation.  Thus, on a term basis, updates to the aggregate values are only made when sufficient changes not using the current value will have an effect on the final weights and the search/ranking process.  This process also distributes the update process over time by only updating a subset of terms at any instance in time.  The third approach is the most accurate.  The weighted values in the database only matter when they are being used to determine items to return from a query or the rank order to return the items.  This has more overhead in that database vector term weights must be calculated dynamically for every query term.  If the system is using an inverted file search structure, this overhead is minor. 	An interesting side effect of maintaining currency in the database for term weights is that the same query over time returns a different ordering of items.  A new word in the database undergoes significant changes in its weight structure from initial introduction until its frequency in the database reaches a level where small changes do not have significant impact on changes in weight values.  	Another issue is the desire to partition an information database based up-on time.  The value of many sources of information vary exponentially based upon the age of an item (older items have less value).  This leads to physically partition-ing the database by time (e.g., starting a new database each year), allowing the us-er to specify the time period to search.  There are issues then of how to address the aggregate variables that are different for the same processing token in each data-base and how to merge the results from the different databases into a single Hit file.	The best environment would allow a user to run a query against multiple different time periods and different databases that potentially use different weighting algorithms, and have the system integrate the results into a single ranked Hit file.  4.3.1.3.6  Problems With the Vector Model	In addition to the general problem of dynamically changing databases and the effect on weighting factors, there are problems with the vector model on as-signment of a weight for a particular processing token to an item.  Each processing token can be viewed as a new semantic topic.  A major problem comes in the vec-tor model when there are multiple topics being discussed in a particular item.  For example, assume that an item has an in-depth discussion of “oil” in “Mexico” and also “coal” in “Pennsylvania.”  The vector model does not have a mechanism to associate each energy source with its particular geographic area.  There is no way to associate correlation factors between terms (i.e., precoordination discussed in Chapter 3) since each dimension in a vector is independent of the other dimen-sions.  Thus the item results in a high value in a search for “coal in Mexico.”	Another major limitation of a vector space is in associating positional in-formation with a processing term.  The concept of proximity searching (e.g., term “a” within 10 words of term “b”) requires the logical structure to contain storage of positional information of a processing term.  The concept of a vector space al-lows only one scalar value to be associated with each processing term for each item.  Restricting searches to subsets of an item has been shown to provide in-creased precision.  In effect this capability overcomes the multi-topical item prob-lem by looking at subsets of an item and thus increasing the probability that the subset is discussing a particular semantic topic.4.3.2 Natural LanguageThe goal of natural language processing is to use the semantic informa-tion in addition to the statistical information to enhance the indexing of the item.  This improves the precision of searches, reducing the number of false hits a user reviews. The semantic information is extracted as a result of processing the lan-guage rather than treating each word as an independent entity.  The simplest output of this process results in generation of phrases that become indexes to an item.  More complex analysis generates thematic representation of events rather than phrases. Statistical approaches use proximity as the basis behind determining the strength of word relationships in generating phrases.  For example, with a proximity constraint of adjacency, the phrases “venetian blind” and “blind Venetian” may appear related and map to the same phrase.  But syntactically and semantically those phrases are very different concepts.  Word phrases generated by natural language processing algorithms enhance indexing specification and provide another level of disambiguation.  Natural language processing can also combine the concepts into higher level concepts sometimes referred to as thematic representations.  One example represents them as concept-relationship-concept triples   (Liddy-93). 4.3.2.1  Index Phrase Generation	The goal of indexing is to represent the semantic concepts of an item in the information system to support finding relevant information.  Single words have conceptual context, but frequently they are too general to help the user find the desired information.  Term phrases allow additional specification and focusing of the concept to provide better precision and reduce the user’s overhead of re-trieving non-relevant items.  Having the modifier “grass” or “magnetic” associated with the term “field” clearly disambiguates between very different concepts.  One of the earliest statistical approaches to determining term phrases proposed by Sal-ton was use of a COHESION factor between terms (Salton-83):COHESIONk,h = SIZE-FACTOR ∗ (PAIR-FREQk,h / TOTFk ∗ TOTFH)where SIZE-FACTOR is a normalization factor based upon the size of the vocabu-lary and PAIR-FREQk,h  is the total frequency of co-occurrence of the pair Termk , Termh in the item collection.  Co-occurrence may be defined in terms of adjacency, word proximity, sentence proximity, etc.  This initial algorithm has been modified in the SMART system to be based on the following guidelines (BUCKLEY-95):any pair of adjacent non-stop words is a potential phraseany pair must exist in 25 or more itemsphrase weighting uses a modified version of the SMART system single term algorithmnormalization is achieved by dividing by the length of the single-term subvector.	Natural language processing can reduce errors in determining phrases by determining inter-item dependencies and using that information to create the term phrases used in the indexing process.  Statistical approaches tend to focus on two term phrases.  A major advantage of natural language approaches is their ability to produce multiple-term phrases to denote a single concept.  If a phrase such as “in-dustrious intelligent students” was used often, a statistical approach would create phrases such as “industrious intelligent” and “intelligent student.”  A natural lan-guage approach would create phrases such as “industrious student,” “intelligent student” and “industrious intelligent student.” 	The first step in a natural language determination of phrases is a lexical analysis of the input.  In its simplest form this is a part of speech tagger that, for example, identifies noun phrases by recognizing adjectives and nouns. Precise part of speech taggers exist that are accurate to the 99 per cent range. Additionally, proper noun identification tools exist that allow for accurate identification of names, locations and organizations since these values should be indexed as phrases and not undergo stemming.  Greater gains come from identifying syntactic and semantic level dependencies creating a hierarchy of semantic concepts. For example, “nuclear reactor fusion” could produce term phrases of “nuclear reactor” and “nuclear fusion.”  In the ideal case all variations of a phrase would be reduced to a single canonical form that represents the semantics for a phrase.  Thus, where possible the phrase detection process should output a normalized form.  For example, “blind Venetian” and “Venetian who is blind” should map to the same phrase. This not only increases the precision of searches, but also increases the frequency of occurrence of the common phrase. This, in turn, improves the likelihood that the frequency of occurrence of the common phrase is above the threshold required to index the phrase. Once the phrase is indexed, it is available for search, thus participating in an item’s selection for a search and the rank associated with an item in the Hit file.   One solution to finding a common form is to transform the phrases into an operator-argument form or a header-modifier form.  There is always a category of semantic phrases that comes from inferring concepts from an item that is non-determinable.  This comes from the natural ambiguity inherent in a language that is discussed in Chapter 1.	A good example of application of natural language to phrase creation is in the natural language information retrieval system at New York University de-veloped in collaboration with GE Corporate Research and Development (Carbal-lo-95).  The text of the item is processed by a fast syntactical process and extract-ed phrases are added to the index in addition to the single word terms.  Statistical analysis is used to determine similarity links between phrases and identification of subphrases.  Once the phrases are statistically noted as similar, a filtering process categorizes the link onto a semantic relationship (generality, specialization, anton-ymy, complementation, synonymy, etc.).	The Tagged Text Parser (TTP), based upon the Linguistic String Grammar (Sager-81), produces a regularized parse tree representation of each sentence reflecting the predicate-argument structure (Strzalkowski-93).  The tagged text parser contains over 400 grammar production rules.  Some examples of the part of speech tagger identification are given in Figure 4.8.	CLASS				EXAMPLESdeterminers 			a, the singular nouns		paper, notation, structure plural nouns			operations, data, processes preposition			in, by, of, foradjective			high, concurrentpresent tense verb		presents, associates		present participal		multiprogramming4.8  Part of Speech TagsThe TTP parse trees are header-modifier pairs where the header is the main con-cept and the modifiers are the additional descriptors that form the concept and eliminate ambiguities. Figure 4.9 gives an example of a regularized parse tree structure generated for the independent clause:The former Soviet President has been a local hero  ever since a Russian tank invaded Wisconsin|assert  perf[HAVE]    verb[BE]      subject         np          noun[President]          t_pos[The]          adj[former]          adj[Soviet]      object         np          noun[hero]          t_pos[a]          adj[local]      adv[ever]      sub_ord         [since]            verb[invade]               subject                 np                   noun[tank]                   t_pos[a]                   adj[Russian]                 object                   np                     noun[Wisconsin]Figure 4.9 TTP Parse TreeThis structure allows for identification of potential term phrases usually based up-on noun identification.  To determine if a header-modifier pair warrants indexing, Strzalkowski calculates a value for Informational Contribution (IC) for each element in the pair.  Higher values of IC indicate a potentially stronger semantic relationship between terms.  The basis behind the IC formula is a conditional probability between the terms. The formula for IC between two terms (x,y) is:IC(x,[x,y]) =   where fx,y is the frequency of (x,y) in the database, nx is the number of pairs in which “x” occurs at the same position as in (x,y) and D(x) is the dispersion pa-rameter which is the number of distinct words with which x is paired.  When IC=1, x occurs only with y (fx,y=nx  and dx = 1).	Nominal compounds are the source of many inaccurate identifications in creating header-modifier pairs.  Use of statistical information on frequency of oc-currence of phrases can eliminate some combinations that occur infrequently and are not meaningful. 	The next challenge is to assign weights to term phrases.  The most popu-lar term weighting scheme uses term frequencies and inverse document frequen-cies with normalization based upon item length to calculate weights assigned to terms.  Term phrases have lower frequency occurrences than the individual terms. Using natural language processing, the focus is on semantic relationships versus frequency relationships.  Thus weighting schemes such as inverse document fre-quency require adjustments so that the weights are not overly diminished by the potential lower frequency of the phrases.  For example, the weighting scheme used in the New York University system uses the following formula for weighting phrases:weight(Phrasei) = (Ci∗log(termf) + C2∗α(N,i))∗IDFwhere α(N,i) is 1 for i<N and 0 otherwise and C1 and C2 are normalizing factors.  The N assumes the phrases are sorted by IDF value and allows the top “N” highest IDF (inverse document frequency) scores to have a greater effect on the overall weight than other terms.4.3.2.2  Natural Language Processing	Section 4.3.2.1 discussed generation of term phrases as indexes. Lexical analysis determining verb tense, plurality and part of speech is assumed to have been completed prior to the following additional processing.  Natural language processing not only produces more accurate term phrases, but can provide higher level semantic information identifying relationships between concepts.  	The DR-LINK system (Liddy-93) and its commercial implementation via Textwise System adds the functional processes Relationship Concept Detectors, Conceptual Graph Generators and Conceptual Graph Matchers that generate high-er level linguistic relationships including semantic and discourse level relation-ships. This system has evolved into the CINDOR system that expanded to natural language (http://www.textwiselabs.com/government/index.html) and cross lan-guage retrieval. This system is representative of natural language based processing systems.  During the first phase of this approach, the processing tokens in the doc-ument are mapped to Subject Codes as defined by the codes in the Longman’s Dictionary of Common English (LDOCE). Disambiguation uses a priori statistical term relationships and the ordering of the subject codes in the LDOCE, which in-dicates most likely assignment of a term to a code.  These codes equate to index term assignment and have some similarities to the concept-based systems dis-cussed in Section 5.4.	The next phase is called the Text Structurer, which attempts to identify general discourse level areas within an item.  Thus a news story may be subdivid-ed into areas associated with EVALUATION (opinions), Main event (basic facts),  and Expectations (Predictions). These have been updated to include Analytical In-formation, Cause/Effect Dimension and Attributed Quotations in the more recent versions of DR-LINK (see http://199.100.96.2 on the Internet).  These areas can then be assigned higher weighting if the user includes “Preference” in a search statement.  The system also attempts to determine TOPIC statement identifiers.  Natural language processing is not just determining the topic statement(s) but also assigning semantic attributes to the topic such as time frame (past, present, and fu-ture).  To perform this type analysis, a general model of the predicted text is need-ed.  For example, news items likely follow a model proposed by van Dijk (Dijk-88).  Liddy reorganized this structure into a News Schema Components consisting of Circumstance, Consequence, Credentials, Definition, Error, Evaluation, Expec-tation, History, Lead, Main Event, No Comment, Previous Event, References and Verbal reaction.  Each sentence is evaluated and assigned weights associated with its possible inclusion in the different components.  Thus, if a query is oriented to-ward a future activity, then, in addition to the subject code vector mapping, it would weight higher terms associated with the Expectation component. 	The next level of semantic processing is the assignment of terms to com-ponents, classifying the intent of the terms in the text and identifying the topical statements.  The next level of natural language processing identifies inter-relationships between the concepts.  For example, there may be two topics within an item “national elections” and “guerrilla warfare.”  The relationship “as a result of” is critical to link the order of these two concepts.  This process clarifies if the elections were caused by the warfare or the warfare caused by the elections.  Sig-nificant information is lost by not including the connector relationships.  These types of linkages are generated by general linguistic cues (words in text) that are fairly general and domain independent. 	The final step is to assign final weights to the established relationships.  The relationships are typically envisioned as triples with two concepts and a rela-tionship between them.  Although all relationships are possible, constructing a system requires the selection of a subset of possible relationships and the rules to locate the relationships.  The weights are based upon a combination of statistical information and values assigned to the actual words used in establishing the link-ages.  Passive verbs would receive less weight than active verbs.	The additional information beyond the indexing is kept in additional data structures associated with each item.  This information is used whenever it is im-plicitly included in a search statement that is natural language based or explicitly requested by the user.4.3.3  Concept Indexing 	Natural language processing starts with a basis of the terms within an item and extends the information kept on an item to phrases and higher level con-cepts such as the relationships between concepts.  In the DR-LINK system, terms within an item are replaced by an associated Subject Code.  Use of subject codes or some other controlled vocabulary is one way to map from specific terms to more general terms.  Often the controlled vocabulary is defined by an organization to be representative of the concepts they consider important representations of their data.  Concept indexing takes the abstraction a level further. Its goal is to gain the implementation advantages of an index term system but use concepts in-stead of terms as the basis for the index, producing a reduced dimension vector space.  By reducing the dimensionality (think of it as the number of words in the “concept language), different synonyms for the same word/concept will be mapped to a single new word (concept vector).  This then helps solve the difference in vocabularies that is one of the core problems in information retrieval.	Rather than a priori defining a set of concepts that the terms in an item are mapped to, concept indexing can start with a number of unlabeled concept classes and let the information in the items define the concepts classes created.  The process of automatic creation of concept classes is similar to the automatic generation of thesaurus classes that will be described in Chapter 6.  The process of mapping from a specific term to a concept that the term represents is complex be-cause a term may represent multiple different concepts to different degrees.  A term such as “automobile” could be associated with concepts such as “vehicle,” “transportation,” “mechanical device,” “fuel,” and “environment.”  The term “au-tomobile” is strongly related to “vehicle,” lesser to “transportation” and much lesser the other terms.  Thus a term in an item needs to be represented by many concept codes with different weights for a particular item.  	An early example of applying a concept approach is the Convectis Sys-tem from HNC Software Inc. (Caid-93, Carleton-95)r approaches for decision software.  The basis behind the generation of the concept approach is a neural network model (Waltz-85).  Context vector representation and its application to textual items are described by Gallant (Gallant-91a, Gallant-91b).  If a vector ap-proach is envisioned, then there is a finite number of concepts that provide cover-age over all of the significant concepts required to index a database of items.  The goal of the indexing is to allow the user to find required information, minimizing the reviewing of items that are non-relevant. In an ideal environment there would be enough vectors to account for all possible concepts and thus they would be or-thogonal in an “N” dimensional vector-space model.  It is difficult to find a set of concepts that are orthogonal with no aspects in common.  Additionally, implemen-tation trade offs naturally limit the number of concept classes that are practical.  These limitations increase the number of classes to which a processing token is mapped.	The Convectis system uses neural network algorithms and terms in a sim-ilar context (proximity) of other terms as a basis for determining which terms are related and defining a particular concept.  A term can have different weights associated with different concepts as described.  The definition of a similar context is typically defined by the number of non-stop words separating the terms.  The farther apart terms are, the less coupled the terms are associated within a particular concept class.  Existing terms already have a mapping to concept classes.  New terms can be mapped to existing classes by applying the context rules to the classes that terms near the new term are mapped.  Special rules must be applied to create a new concept class.  Example 4.10 demonstrates how the process would work for the term “automobile.”	TERM:	automobile					Weights for associated concepts:	Vehicle				     	       .65	Transportation				       .60	Environment				       .35	Fuel					       .33	Mechanical Device			       .15	Vector Representation Automobile:  (.65,..., .60, ..., .35, .33, ... , .15)Figure 4.10 Concept Vector for Automobile	Using the concept representation of a particular term, phrases and com-plete items can be represented as a weighted average of the concept vectors of the terms in them.  The algorithms associated with vectors (e.g., inverse document frequency) can be used to perform the merging of concepts.	Latent Semantic Indexing (LSI) is another approach to defining concept vectors and it is the more common approach in commercial systems. GOOGLE, when it acquired Applied Semantics, recognized that to better deliver what a user is looking for, the limitation of only returning results based upon the users query terms is too restrictive.  Another major commercial company (Autonomy and their IDOL system) also claim to be using concept indexing, but it is based upon an Naïve Baysean approach.  Latent Semantic Indexing’s assumption is that there is an underlying or “latent” structure represented by interrelationships between words (Deerwester-90, Dempster-77, Dumais-95, Gildea-99, Hofmann-99).  The index contains representations of the “latent semantics” of the item.  The large term-document matrix is decomposed into a small set (e.g., 100-300) of orthogonal factors which use linear combinations of the factors (concepts) to approximate the original matrix. Latent Semantic Indexing uses singular-value decomposition to model the associative relationships between terms similar to eigenvector decomposition and factor analysis (see Cullum-85).	As described in Chapter 2, a rectangular matrix can be decomposed into the product of three matrices.  Let X be a mxn matrix such that:X = T0•S0•D0′ where T0 and D0 have orthogonal columns and are m x r and r x n matrices, S0 is an r x r diagonal matrix and r is the rank of matrix X.  This is the singular value decomposition of X.  The k largest singular values of S0 are kept along with their corresponding columns in T0 and D0 matrices, the resulting matrix:  = Tn•Sn•Dn′is the unique matrix of rank k that is closest in least squares sense to X.  The ma-trix , containing the first k independent linear components of the original X  represents the major associations with noise eliminated.  If you consider X to be the term-document matrix (e.g., all possible terms being represented by columns and each item being represented by a row), then truncated singular value decomposition can be applied to reduce the dimmension-ality caused by all terms to a significantly smaller dimensionality that is an ap-proximation of the original X:X = U•SV•V′where u1 ... uk and v1 ... vk are left and right singular vectors and sv1 ... svk are sin-gualr values.  A threshold is used against the full SV diagnonal matrix to deter-mine the cutoff on values to be used for query and document representation (i.e., the dimensionality reduction).  Hofmann has modified the standard LSI approach using addional formalism via Probabilistic Latent Semantic Analysis (Hofmann-99). With so much reduction in the number of words, closeness is determined by patterns of word usage versus specific co-locations of terms.  This has the ef-fect of a thesaurus in equating many terms to the same concept.  Both terms and documents (as collections of terms) can be represented as weighted vectors in the k dimensional space.  The selection of k is critical to the success of this procedure.  If k is too small, then there is not enough discrimination between vectors and too many false hits are returned on a search.  If k is too large, the value of Latent Se-mantic Indexing is lost and the system equates to a standard vector model.  The details of how to create the vectors for Latent Semantic Indexing was described in detail in Chapter 2.  We will now use that derivation and relate it to creating a textual index.  We will redefine the original matrix A from Chapter 2 to be a matrix representing a textual input and use an example shown in Grossman and Fieder book on Information Retrieval (Grossman and Fieder 2004). In Chapter 5 we will show how a query operates against the index.D1:  “Shipment of gold damaged in a fire”D2:  “Delivery of silver arrived in a silver truck”D3:  “Shipment of gold arrived in a truck”That produces the matrix we used in the LSI example in Chapter 2:	D1	D2	D3a	1	1	1arrived	0	1	1damaged	1	0	0delivery	0	1	0fire	1	0	0gold	1	0	1in	1	1	1of	1	1	1Shipment	1	0	1silver	0	2	0truck	0	1	1The goal is to reduce the dimensionality of the information space and that is ac-complished by dropping the lowest singular values of the S matrix.  The S matrix calculated in Chapter 2 was: There are only three values so in this example the lowest value (1.2737) is dropped.  The U matrix can be thought of as a mapping of the original vocabulary (rows) to the new concept vector vocabulary (columns). Instead of each column representing a document each column represents a concept vector and the rows are mapping how much each processing token in the original document space is repre-sented in each of the concept vectors. The column associated with the lowest sin-gular value is dropped.  In the Singular value diagonal matrix the row in this ex-ample (or rows) with lowest values are dropped.  The V matrix can be considered a mapping of the documents to the concept vectors which are like the new vocabu-lary with the concept vectors being the rows.  Thus the bottom row in this case is dropped.  This yields the following index space starting with U:  to  The Singular Value diagonal matrix has reduced:   to    And the V new index has reduced to: to  The new U matrix will be used to transform a user’s query into a query in the con-cept vectors.  That will then be searched against the reduced V matrix to get doc-ument answers.4.4  Automatic Indexing of Multimedia	The study of Information Retrieval Systems is not complete unless a basic understanding of items that are multimedia are also considered.  They are becoming too common in the electronic communications and processes due to major advances and simplifications in new technologies.  The users will expect to be able to get similar accuracy in searching for multimedia items that they get in textual items.  There is a growing commercial need for such tools to help users to organize and find information in the large qualities of multimedia they are storing on their local systems as well as finding it on the Internet.  A good example is the new capability to have your iPhone listen to a few seconds of music and the system will then tell you the song and author (and of course how to purchase it).  The demands for accurate multimedia information retrieval will continue to grow as a new commercial area with pressure in developing better search techniques.4.4.1 Introduction to Mutlimedia Indexing	Indexing associated with multimedia differs from the previous discus-sions of indexing.  The automated indexing takes place in multiple transitions of the information versus just a direct conversion to the indexing structure.  In some cases (e.g., analog video, audio) the input modality needs to be converted to a digital format before the ingestion process can begin as the first transition.  Once digital, algorithms are applied to the digital structure to extract the unit of pro-cessing of the different modalities that will be used to represent the item.  In an abstract sense this could be considered the location of a processing token in the modality.  This unit will then undergo the final processing that will extract the searchable features that represent the item.	There are many different modalities other than text that could be dis-cussed for indexing.  This book will focus on the three major modalities that are most common to all users; audio, video and images.  Modalities such as maps and geographic objects, although becoming more common because of GOOGLE Earth, are not as prevalent as the three that will be discussed.  Image search has been around for a long time.  The first sophisticated search techniques were funded by the Government to focus on satellite imagery.  When the Internet started to become popular attempts were made to be able to search for images on the Inter-net (e.g., IBM, Virage and Alta Vista).  But it was quickly found that searching the text associated with images was more accurate than in searching the images them-selves.  Now searching the images is starting to become more feasible and will provide better results that just the text.  When audio is discussed what is of interest is primarily when there is speech in the audio or other sounds of interest.  Alt-hough the earliest commercial information retrieval for audio was focused on val-idating how often advertisements or songs were played to be sure contract com-mitments were met.  Searching for what is being spoken, for example on news IPTV Internet broadcasts is now becoming important.  When video is discussed what is being considered is a modality that has time synchronized video and audio subtracks.  There also could be closed captioning and Teletext that imbedded in the video that will be discussed.  In current television news video there are addi-tional independent information streams of text on other topics running at the bot-tom of many news sources imbedded in the video.  Video is a super set of audio and images because it is composed of an audio track and the video which uses im-age search technologies.  Search of audio and search of images will be discussed first.  The techniques discussed for them will be applicable to video with addition-al constraints possible.4.4.2  Audio Indexing	Indexing of audio may first have to start with conversion of an analog audio into digital format to be processed.  This first conversion is critical because any noise introduced in the conversion process can affect the capability to process the digital form and identify the spoken word or other sounds of interest.  The voice range goes up to 16 KHz and typically 16 bit sampling is required as a minimum.  Thus the analog audio is converted to a minimum 64Kbits/second (16KHz times 16 bits) as the minimum encoding.  It is very import that the audio levels are watched so that they do not “overdrive” the encoding process.  If the audio levels are too strong it will force the audio signal to be outside the encoders range and the signal can be “clipped” or cut off losing the shape and distinction between samples.  The other issue is that the digitization processors can introduce noise into the audio (e.g., think of it as a buzz sound) that can make it harder to recognize the original sounds.  Thus care is needed in the initial first phase of con-verting the audio to digital format.	Audio indexing is based upon phonemes and uses Hidden Markov models when speech to text (automatic speech recognition – ASR) is the search approach.  When search of a phonetic index is used then the Hidden Markov Model is not so important.  To be able to build an audio index the first step is to train a system on a particular audio source (i.e. language/dialect).  The first step in the training process is to define the phonemes for the language to the system.  The phonemes for a language are equivalent to the alphabet for a written language.  There is an international standard (International Phonetic Alphabet - IPA) that is used to define the phonemes for a language, but there is usually disagreement on the exact number of phonemes for a language. Phonemes are the smallest sounds that are needed to differentiate between words in a language.  For English there are 42-46 phonemes (there is not a consensus on the exact number).  One of the languages with the most number of phonemes is an African language spoken in Botswana that has over 112 phonemes.  One of the languages with the fewest number of phonemes is Rutukas spoken in New Guinea that has only 11 phonemes.  Ideally you would like for a system to learn the phonemes from the training data.  But typically the phonemes are defined are manually defined before the training data is used to develop the Hidden Markov Model.  Statistical language models are used that look at the probabilities of the juxtaposition of phonemes, which sounds are likely to follow which sounds, and the probability of occurrence of words and their juxtaposition. Once the audio arrives at the indexer, it will match the audio input to the language the system has been trained on.  The training data set is typically 60 to 100 hours of audio along with a transcription of the audio. There are training data sets available at the Linguistic Data Consortium (LDC) at the University of Penn-sylvania.  But for new languages, they need to be developed (e.g., Appen from Australia will do it).  The transcription of the 60-100 hours of audio is a marked up transcription of the words spoken.  Marked up in the sense it shows the start and stop time in the audio track for each word (although modern systems can now automatically determine the stop time).  When the process first started many years ago all sounds were included in the marked up transcription (e.g., breathing, “hems”, clear throat, ect.).  Over time models for all these other sounds have been developed that can be used in processing new languages, so they are no longer needed.  The training data serves two major purposes.  First, by having the words correlated to the audio locations for the words and having the theoretical phonetic definitions that map phonemes to characters (or glyphs) in the vernacular language, the system can improve the mapping (model) of the particular glyph to the actual audio sounds for that phoneme.  This will improve the accuracy as new unmarked data is processed.  Second is that it allows the system to automatically develop a statistical model of the frequency of occurrence of phonemes in the language.  In addition to the frequency of occurrence of the phonemes it also develops the frequency of occurrence of the trinemes (three phonemes together) and quinemes (four phonemes together) that are used in many of the word detection algorithms used in Automatic Speech Recognition. When ASR is the goal, in addition to the training to recognize phonemes by an audio training set, it is also useful to get a large corpus of items in the vernacular.  That large corpus (preferable millions of words) is used to develop a model of when one word follows another.	There are two major indexing approaches to searching the audio called text based Continuous Speech Recognition (CSR) also called Automatic Speech Recognition (ASR) and Phonetic Search.  In text based ASR, the audio is proc-essed and the words are recognized in the audio source and a textual transcription in the vernacular of the audio source is made.  Once the audio has been transformed into text, it can be processed using all the algorithms discussed above for text.  The training data will be used to create a Hidden Markov Model (HMM - see Chapter 2) that represents the audio model of the language constructed from the training data. The HMM is mapped to a dictionary of words so that the sounds which are considered the observable output of the HMM are used with knowledge of probability of the phonemes, trinemes and quinemes that define the state transitions and probability of the set of states to estimate which word is in the audio.    As each word is identified, the physical location of the word is also output typically in an XML data stream to be used in the results display interface (see Chapter 7).	The text will contain a significant number of word errors. Audio tran-scription maps the phonemes that are in an audio item to the words most closely approximating those phonemes in a dictionary.  Good audio transcription of broadcast news still has 10% of the words in error and conversational speech has 40% or more of the words in error.  These will be valid words but the wrong word.  One mechanism to reduce the impact of the missing words in conversational speech is to use the existing database to expand the document.  This is accom-plished by using the transcribed document as a query against the existing database, selecting a small number of the highest ranked results, determining the most im-portant (highest frequency) words across those items and adding those words to the original document.  The new document will then be normalized and re-weighted based upon the added words (Singhal-99).  This technique reduced the losses in retrieval effectiveness from 15-27% to 7-13% when the audio transcrip-tions had high errors (40% or more).  It has marginal benefit when the transcrip-tion has errors in the 15% range.  Thus it is useful when working with conversa-tional speech but marginally useful against broadcast news and professional speakers.	The Phonetic Indexing approach works with sounds by identifying the phonemes in the in the audio stream.  It then creates an index based upon the pho-netic sounds of each word. The proprietary aspect of a phonetic search system is how the phoneme index is created, stored and searched.  A Phonetic dictionary is required to take the search input typically, in textual form from a user, and convert it to a phonetic search string.	The comparison of the two approaches shows each has its major ad-vantages and disadvantages. Both approaches are sensitive to the training data provided.  Since the training data is used to adjust the acoustic model of the pho-nemes, the developed system will work more accurately against similar audio (e.g., from that source or a similar source) then from new sources.  Thus in the training data diversity of speakers and examples of the types of audio to be processed can make an improvement in the overall accuracy of the operational system.  In most systems using either approach accuracies of around 90 per cent recognition can be achieved for broadcast news and 65 per cent for conversational speech.  The accuracy is very sensitive to dialects and individuals.  Conversational speech is difficult for both methods because of the typical overlap of speakers (both people speaking at the same time) and dialects and accents of the speakers.  Also people have a tendency to speak faster when in conversation versus when doing formal speaking. 	In terms of processing performance, the Phonetic Index approach can process 10 or more different sources on a single processor and the phonetic index is about 10 per cent the size of the audio input.  ASR can now handle 6 or more different audio streams on a single processor.  The size of the index is based upon the text produced versus the size of the audio input.  The ASR approach converts the audio to text which then uses standard text indexing methods.  Since it is based upon the dictionary created during the training process it has always suffered from a problem called “out of vocabulary – OOV” words that were not part of the initial dictionary.  This deficiency of being limited to the vocabulary in the training set has the serious limitation on new names of people and organizations that will be mapped to the wrong word.  More recently ASR systems are allowing the user to dynamically enter new vocabulary terms which are not merged into the HMM model but made recognizable as an adjunct data set.  Phonetic search is based upon phonemes and not words.  Thus new words or names are indexed as well as those in the training data set.  This is one of strongest advantages of phonetic search over text based ASR.  Since Phonetic search is based upon phonemes, shorter words that have fewer phonemes suffer from false hits where the acoustic model of the few phonemes of the search term is a subset of a phonetic model of longer words.  For example if you have a search term “ray” you will possibly get hits on the word “tray”.  ASR creates a text word based system that avoids that issue. The other major issue of a phonetic search system is related to how the output (hit list) is shown.  For the text ASR system the hit list can contain textual snippets of the text around the hit term which places each hit in context. For a phonetic search system only the search term and the position in the audio can be shown. Thus the user does not have the textual context of the hit to determine if the hit is in error or something they want to listen too.  Thus the user has to open the item and listen to the audio to determine if the audio meets their information need. This will be discussed more in Chapter 7.	In addition to recognizing words in a language, audio models can be used to recognize other information.  Audio models can recognize what language a par-ticular audio input is in.  This is a precursor to feeding that audio to a particular indexer that has that audio language model.  With a little amount of training a sys-tem can recognize who is speaking (speaker identification).  The only issue comes with the number of speakers that have been modeled.  As the number gets into the hundreds (e.g., over 300) the error rates begin to grow because of too many simi-lar models.  Also other unique sounds can be modeled and identified.  For example gunfire and explosions have been identified that assist in alerting a user when those sounds are discovered in news sources.	For audio the processing tokens are either the phonemes if you are using a phonetic search system or the transcribed words if you are using an ASR system.4.4.3  Image IndexingImages are already in image form.  Images are made up of pixels (picture elements – always wondered why they are not called picels). A pixel is the small-est processing aspect of the image.  A pixel has a location and has attributes such as color and intensity (e.g., grey scale). The number of bots per pixel defines the display options.  For example 2 bits allow for just black/white, 8 bits are used to define gray scale or super VGA color scale, and 24 bit for true color (3 8-bit subpixels for red, green and blue.)  The processing tokens for indexing images can be accomplished at the raw data level (e.g., the aggregation of raw pixels), the feature level distinguishing primitive attributes such as color and luminance, and at the semantic level where meaningful objects are recognized (e.g., an airplane in the image/video frame or recognizing text within the image).  In the aggregation of pixel information the image is partitioned into sub-areas and all of the pixel information in a sub-area will be combined to create a single value for that sub-area.  The heuristics is in the way that the pixel information is used to create a value for the sub-area.  It could be combined using Fourier transforms or it could use averaging techniques such as Shannon’s entropy law.  The smaller the size of the sub-area the more detailed the indexing locating more detailed features but the more complex the search. At the primitive attribute level the processing tokens will define a vector that represents the different features associated with that frame.  Each dimension of the vector represents a different feature level aspect of the frame.  The vector then becomes the unit of processing in the search system.  The three primary primitive aspects are the color, the “texture” and shapes within the image.  Colors are an attribute of each pixel and the processing tokens can be created by defining a color histogram for different portions of the image.  Texture is a more difficult aspect of the image.  There is no universal definition of texture and the actual value of texture is a relative value.  It is sometimes described as the two dimensional grey level variations between adjacent pixels and can be discussed in terms of contrast, coarseness and directionality.  The texture can be defined in terms of “texels – texture elements”  associated with a 3- dimension consideration of the image (i.e., texture overlay on the 3-dimensional definition of the two dimensional image).  Texture or texels are relative because the nearness of the viewer (e.g., focal point) can redefine the visibility of the texture.  If you get up close to an image you can see a lot more detail of the adjacent pixels and thus can see a lot more texture.  As you move further away the textures start to blend together as you lose the distinction.  The texture can be defined statistically as grey tone spatial distributions or its possible to set up a series of “production rules” that define structural texels.  Like the color the texels can be grouped as arrays and defined as elements in a vector describing the image.The final primitive feature of an image is basic shapes.  They can be lo-calized by adjacent pixels that vary in color or grayness.  It’s possible by looking at variations to define the basic shapes that are found in the image.  These also can be made part of the vector.  The resultant vector and its individual elements be-come the processing tokens that define the image and can be used to search for other images that have the same characteristics.  Those that are similar could be copies or the same information in the current image.  This leads to a lot of errors in search.The semantic level tries to get closer to indexing the actual content of the image rather than just the characteristics of the image.  The major semantic as-pects of an image are the actual objects in the image and the text within an image.  Text is far more important when inputs such as video and television news are con-sidered where text plays a major role in defining what is being shown or providing additional news.   For example television usually places in text the name of an im-portant person that the news is showing.  It also has streams of text on other sub-jects across the bottom of the display.  There are two approaches for indexing the text in an image by segment.  Both start with the initial process of identifying text in the image.  This is accomplished by recognizing the overall shape of text and segmenting that portion of the image for additional processing.  The first approach given the text has been segmented is to segment it down to the character level and then applying Optical Character Recognition (OCR) or Optical Writing Recogni-tion) algorithms to translate the image to text that is then searchable.  The text is extracted and associated with that image.  The second approach is to do more extensive shape definition for that textual portion and store it as part of the processing tokens for that image.  Then when a user searches for Text in an image the users textual search is treated similarity as an image and thus the matching is image of text against image of text.  This has achieved very high recognition rates (e.g., See the PixServe system).The other semantic level indexing comes from better recognition of se-mantic items in the image.  It is an extension of the shapes that have been recog-nized as basic elements in defining semantic (thematic) objects.  For example the image can be broken into foreground and background.  Specific objects can be de-fined by a meta-language defining the shapes.  A flower is an oval/circular color-ful object connected by a rectangular/long oval shape major object typically greenish in color.  These semantic objects can then be used as part of the pro-cessing tokens associated with an image and can be specified by the users query.When working with the primitive elements that are extracted as pro-cessing tokens a vector – or specific fielded data architecture can be used as the index.  When the semantic information is being used as processing tokens the searchable index is stored as an XML structure that can be loaded into memory and searched.4.4.4 Video IndexingVideo can be a simple construct that only contains the video and audio associated with that video.  But when looking at television broadcasting there are additional data that can be imbedded in the television signal.  In particular there is closed captioning and Teletext.  Teletext is often an independent textual stream that has information not related to the audio and video and as such it can be extracted and treated as a new input textual stream. Teletext was developed based upon requirements of the British Broadcasting Corporation (BBC) in 1970 as a mechanism to include subtitles in television transmission. Teletext information is broadcast in the vertical blanking interval (i.e., vbi is the time between frames in a raster scan TV signal when the scan is being reset back to the top to start the next scan) between image frames in a broadcast television signal.  Closed captioning is an attempt by a human transcriber to capture what is being said in the audio portion of the video. It is encoded also in the vertical blanking interval.  It is similar to the automatic speech recognition described above for audio except it is being done by a human.  This makes it errorful in an unpredictable fashion (i.e., whereas it is possible to predict many of the types of errors in ASR).  But it is closely correlated to the audio and the video, similar to ASR.  The correlation between the audio, the video and the closed captioning is the relative time within the video.  It is possible to have all three independent but correlated channels of information from a television broadcast.  More recently tel-evision news channels have been running within the image another independent banner of text at the bottom of the screen that describes additional items of interest that have nothing to do with what is currently being shown and discussed.  Recog-nizing that video stream and extracting it as an independent text channel of information is a new technology currently being developed.  Although there is occasionally text on images, text also plays a major role in television where usually an important person who is shown will have their name in text underneath them.  Also text is used in the images to describe what is going on (e.g., terrorist Alert”).  All of that text provides a real rich information searchable area different than in just searching the image itself.  Another useful thing found in television is the display of logos which indicate what organization is being discussed and what television station is broadcasting.Each of the channels has its own index as described in this chapter.  The video is really a series of images. Image indexing software described above can be set to periodically (e.g., once a second) capture a frame (which is equivalent to an image) and index that image.  The audio can be processed as described in audio indexing.  The closed captioning can be captured as a stream of text.The audio indexing carries with the index an offset into where the text being transcribed or indexed exists within the video.  The same is true of the in-dexing of the image.  Both of these are done to help in the display of search results to allow a user to jump to the location of a hit search term in the television stream.  Closed captioning can also come with offsets into the television stream.  Where there has been minimal research is how to do a multi-modality search across all three of these channels (image, audio and closed captioning) and determine a hit from it.  In this case the user could create a query with search terms against all three of the channels.  The system would correlate the hits in each channel to when they occurred in relative time since the start of the video.  Then if the hits were within a time window (e.g., with 3 seconds of each other) the hit would be reported.  Thus you could set up a query that is looking for a Burning building (image search) where the closed captioning or audio transcription are discussing “terrorism”.  Since it is typically a different company/product that creates the search index for each of the channels, there has been minimal commercial effort and little research in the academic community on this more complex integrated search capability.Another example of a multimedia integrated search would be for textual items that have multimedia links or images within them.  There are two main mechanisms that are used, positional and temporal.  Positional is used when the modalities are interspersed in a linear sequential composition.  For example a doc-ument that has images or audio inserted can be considered a linear structure and the only relationship between the modalities will be the juxtaposition of each mo-dality.  This would allow for a query that would specify location of an image of a boat within one paragraph of "Cuba and refugees".  In this case position is used in lieu of time as the mechanism that is used to synchronize the different modalities.  To accomplish either type of integrated search the index must include a time-offset parameter versus a physical displacement.  The above examples use proxim-ity to increase precision based upon time concurrency (or ranges) or physical proximity.4.5	SummaryAutomatic indexing is the preprocessing stage allowing search of items in an In-formation Retrieval System.  Its role is critical to the success of searches in find-ing relevant items.  If the concepts within an item are not located and represented in the index during this stage, the item is not found during search.  Some tech-niques allow for the combinations of data at search time to equate to particular concepts (i.e. postcoordination).  But if the words are not properly identified at in-dexing time and placed in the searchable data structure, the system can not com-bine them to determine the concept at search time.  If an inefficient data structure is selected to hold the index, the system does not scale to accommodate large numbers of items. 	The steps in the identification of the processing tokens used in the index process were generally discussed in Chapter 3.  This chapter focused on the spe-cific characteristics of the processing tokens to support the different search tech-niques.  There are many ways of defining the techniques.  All of the techniques have statistical algorithmic properties.  But looking at the techniques from a con-ceptual level, the approaches are classified as statistical, natural language and con-cept indexing.  Hypertext linkages are placed in a separate class because an algo-rithm to search items that include linkages has to address dependencies between items.  Normally the indexing of processing tokens is restricted to an item.  The next item may use some corpus statistics that changed by previous items, but does not consider a tight coupling between items.  Hypertext linkage could in effect in-dicate that one item may be considered an extension of another, which should af-fect the concept identification and representation process.	Of all the statistical techniques, an accurate probabilistic technique would have the greatest benefit in the search process.  Unfortunately, identification of consistent statistical values used in the probabilistic formulas has proven to be a formidable task.  The assumptions that must be made significantly reduce the ac-curacy of the search process.  Vector techniques have very powerful representa-tions and have been shown to be successful.  But they lack the flexibility to repre-sent items that contain many distinct but overlapping concepts.  Bayesian techniques are a way to relax some of the constraints inherent in a pure vector ap-proach, allowing dependencies between concepts within the same item to be rep-resented.  Most commercial systems do not try to calculate weighted values at in-dex time.  It is easier and more flexible to store the basic word data for each item and calculate the statistics at search time.  This allows tuning the algorithms with-out having to re-index the database.  It also allows the combination of statistical and traditional Boolean techniques within the same system.	Natural language systems attempt to introduce a higher level of abstrac-tion indexing on top of the statistical processes.  Making use of rules associated with language assist in the disambiguation of terms and provides an additional layer of concepts that are not found in purely statistical systems.  Use of natural language processing provides the additional data that could focus searches, reduc-ing the retrieval of non-relevant items. The tendency of users to enter short queries may reduce the benefits of this approach. 	Concept indexing is a statistical technique whose goal is to determine a canonical representation of the concepts.  It has been shown to find relevant items that other techniques miss.  In its transformation process, some level of precision is lost.  The analysis of enhanced recall over potential reduced precision is still under investigation.	Indexing of multimedia introduces an initial first step that is needed to transform the original multimedia into a format  from which indexable data can be derived. EXERCISES: 